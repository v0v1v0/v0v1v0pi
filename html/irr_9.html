<div class="container">

<table style="width: 100%;"><tr>
<td>kappam.fleiss</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Fleiss' Kappa for m raters</h2>

<h3>Description</h3>

<p>Computes Fleiss' Kappa as an index of interrater agreement between m raters on categorical data. Additionally, category-wise Kappas could be computed.
</p>


<h3>Usage</h3>

<pre><code class="language-R">kappam.fleiss(ratings, exact = FALSE, detail = FALSE)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>ratings</code></td>
<td>
<p>n*m matrix or dataframe, n subjects m raters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>exact</code></td>
<td>
<p>a logical indicating whether the exact Kappa (Conger, 1980) or the Kappa described by Fleiss (1971) should be computed.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>detail</code></td>
<td>
<p>a logical indicating whether category-wise Kappas should be computed</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Missing data are omitted in a listwise way.<br>
The coefficient described by Fleiss (1971) does not reduce to Cohen's Kappa (unweighted) for m=2 raters. Therefore, the exact Kappa coefficient, which is slightly higher in most cases, was proposed by Conger (1980).<br>
The null hypothesis Kappa=0 could only be tested using Fleiss' formulation of Kappa.
</p>


<h3>Value</h3>

<p>A list with class '"irrlist"' containing the following components:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>$method</code></td>
<td>
<p>a character string describing the method applied for the computation of interrater reliability.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$subjects</code></td>
<td>
<p>the number of subjects examined.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$raters</code></td>
<td>
<p>the number of raters.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$irr.name</code></td>
<td>
<p>a character string specifying the name of the coefficient.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$value</code></td>
<td>
<p>value of Kappa.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$stat.name</code></td>
<td>
<p>a character string specifying the name of the corresponding test statistic.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$statistic</code></td>
<td>
<p>the value of the test statistic.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$p.value</code></td>
<td>
<p>the p-value for the test.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>$detail</code></td>
<td>
<p>a table with category-wise kappas and the corresponding test statistics.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Matthias Gamer</p>


<h3>References</h3>

<p>Conger, A.J. (1980). Integration and generalisation of Kappas for multiple raters. Psychological Bulletin, 88, 322-328.<br><br>
Fleiss, J.L. (1971). Measuring nominal scale agreement among many raters. Psychological Bulletin, 76, 378-382.<br><br>
Fleiss, J.L., Levin, B., &amp; Paik, M.C. (2003). Statistical Methods for Rates and Proportions, 3rd Edition. New York: John Wiley &amp; Sons.
</p>


<h3>See Also</h3>

<p><code>kappa2</code>,
<code>kappam.light</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(diagnoses)
kappam.fleiss(diagnoses)               # Fleiss' Kappa
kappam.fleiss(diagnoses, exact=TRUE)   # Exact Kappa
kappam.fleiss(diagnoses, detail=TRUE)  # Fleiss' and category-wise Kappa

kappam.fleiss(diagnoses[,1:4])         # Fleiss' Kappa of raters 1 to 4
</code></pre>


</div>