<div class="container">

<table style="width: 100%;"><tr>
<td>johnson_neyman</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Calculate Johnson-Neyman intervals for 2-way interactions</h2>

<h3>Description</h3>

<p><code>johnson_neyman</code> finds so-called "Johnson-Neyman" intervals for
understanding where simple slopes are significant in the context of
interactions in multiple linear regression.
</p>


<h3>Usage</h3>

<pre><code class="language-R">johnson_neyman(
  model,
  pred,
  modx,
  vmat = NULL,
  alpha = 0.05,
  plot = TRUE,
  control.fdr = FALSE,
  line.thickness = 0.5,
  df = "residual",
  digits = getOption("jtools-digits", 2),
  critical.t = NULL,
  sig.color = "#00BFC4",
  insig.color = "#F8766D",
  mod.range = NULL,
  title = "Johnson-Neyman plot",
  y.label = NULL,
  modx.label = NULL
)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>model</code></td>
<td>
<p>A regression model. It is tested with <code>lm</code>, <code>glm</code>, and
<code>svyglm</code> objects, but others may work as well. It should contain the
interaction of interest. Be aware that just because the computations
work, this does not necessarily mean the procedure is appropriate for
the type of model you have.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>pred</code></td>
<td>
<p>The predictor variable involved in the interaction.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>modx</code></td>
<td>
<p>The moderator variable involved in the interaction.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vmat</code></td>
<td>
<p>Optional. You may supply the variance-covariance matrix of the
coefficients yourself. This is useful if you are using robust standard
errors, as you could if using the <span class="pkg">sandwich</span> package.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alpha</code></td>
<td>
<p>The alpha level. By default, the standard 0.05.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>plot</code></td>
<td>
<p>Should a plot of the results be printed? Default is <code>TRUE</code>.
The <code>ggplot2</code> object is returned either way.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>control.fdr</code></td>
<td>
<p>Logical. Use the procedure described in Esarey &amp; Sumner
(2017) to limit the false discovery rate? Default is FALSE. See details
for more on this method.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>line.thickness</code></td>
<td>
<p>How thick should the predicted line be? This is
passed to <code>geom_path</code> as the <code>size</code> argument, but because of the way the
line is created, you cannot use <code>geom_path</code> to modify the output plot
yourself.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>df</code></td>
<td>
<p>How should the degrees of freedom be calculated for the critical
test statistic? Previous versions used the large sample approximation; if
alpha was .05, the critical test statistic was 1.96 regardless of sample
size and model complexity. The default is now "residual", meaning the same
degrees of freedom used to calculate p values for regression coefficients.
You may instead choose any number or "normal", which reverts to the
previous behavior. The argument is not used if <code>control.fdr = TRUE</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>digits</code></td>
<td>
<p>An integer specifying the number of digits past the decimal to
report in the output. Default is 2. You can change the default number of
digits for all jtools functions with
<code>options("jtools-digits" = digits)</code> where digits is the desired
number.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>critical.t</code></td>
<td>
<p>If you want to provide the critical test statistic instead
relying on a normal or <em>t</em> approximation, or the <code>control.fdr</code> calculation,
you can give that value here. This allows you to use other methods for
calculating it.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>sig.color</code></td>
<td>
<p>Sets the color for areas of the Johnson-Neyman plot where
the slope of the moderator is significant at the specified level. <code>"black"</code>
can be a good choice for greyscale publishing.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>insig.color</code></td>
<td>
<p>Sets the color for areas of the Johnson-Neyman plot where
the slope of the moderator is insignificant at the specified level. <code>"grey"</code>
can be a good choice for greyscale publishing.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>mod.range</code></td>
<td>
<p>The range of values of the moderator (the x-axis) to plot.
By default, this goes from one standard deviation below the observed range
to one standard deviation above the observed range and the observed range
is highlighted on the plot. You could instead choose to provide the
actual observed minimum and maximum, in which case the range of the
observed data is not highlighted in the plot. Provide the range as a vector,
e.g., <code>c(0, 10)</code>.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>title</code></td>
<td>
<p>The plot title. <code>"Johnson-Neyman plot"</code> by default.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>y.label</code></td>
<td>
<p>If you prefer to override the automatic labelling of the
y axis, you can specify your own label here. The y axis represents a
<em>slope</em> so it is recommended that you do not simply give the name of the
predictor variable but instead make clear that it is a slope. By default,
"Slope of [pred]" is used (with whatever <code>pred</code> is).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>modx.label</code></td>
<td>
<p>If you prefer to override the automatic labelling of
the x axis, you can specify your own label here. By default, the name
<code>modx</code> is used.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The interpretation of the values given by this function is important and not
always immediately intuitive. For an interaction between a predictor variable
and moderator variable, it is often the case that the slope of the predictor
is statistically significant at only some values of the moderator. For
example, perhaps the effect of your predictor is only significant when the
moderator is set at some high value.
</p>
<p>The Johnson-Neyman interval provides the two values of the moderator at
which the slope of the predictor goes from non-significant to significant.
Usually, the predictor's slope is only significant <em>outside</em> of the
range given by the function. The output of this function will make it clear
either way.
</p>
<p>One weakness of this method of probing interactions is that it is analogous
to making multiple comparisons without any adjustment to the alpha level.
Esarey &amp; Sumner (2017) proposed a method for addressing this, which is
implemented in the <code>interactionTest</code> package. This function implements that
procedure with modifications to the <code>interactionTest</code> code (that package is
not required to use this function). If you set <code>control.fdr = TRUE</code>, an
alternative <em>t</em> statistic will be calculated based on your specified alpha
level and the data. This will always be a more conservative test than when
<code>control.fdr = FALSE</code>. The printed output will report the calculated
critical <em>t</em> statistic.
</p>
<p>This technique is not easily ported to 3-way interaction contexts. You could,
however, look at the J-N interval at two different levels of a second
moderator. This does forgo a benefit of the J-N technique, which is not
having to pick arbitrary points. If you want to do this, just use the
<code>sim_slopes</code> function's ability to handle 3-way interactions
and request Johnson-Neyman intervals for each.
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>bounds</code></td>
<td>
<p>The two numbers that make up the interval.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>cbands</code></td>
<td>
<p>A dataframe with predicted values of the predictor's slope
and lower/upper bounds of confidence bands if you would like to make your
own plots</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>plot</code></td>
<td>
<p>The <code>ggplot</code> object used for plotting. You can tweak the
plot like you could any other from <code>ggplot</code>.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Jacob Long <a href="mailto:jacob.long@sc.edu">jacob.long@sc.edu</a>
</p>


<h3>References</h3>

<p>Bauer, D. J., &amp; Curran, P. J. (2005). Probing interactions in fixed and
multilevel regression: Inferential and graphical techniques.
<em>Multivariate Behavioral Research</em>, <em>40</em>(3), 373-400.
<a href="https://doi.org/10.1207/s15327906mbr4003_5">doi:10.1207/s15327906mbr4003_5</a>
</p>
<p>Esarey, J., &amp; Sumner, J. L. (2017). Marginal effects in interaction models:
Determining and controlling the false positive rate. Comparative Political
Studies, 1â€“33. Advance online publication.
<a href="https://doi.org/10.1177/0010414017730080">doi:10.1177/0010414017730080</a>
</p>
<p>Johnson, P.O. &amp; Fay, L.C. (1950). The Johnson-Neyman technique, its theory
and application. <em>Psychometrika</em>, <em>15</em>, 349-367.
<a href="https://doi.org/10.1007/BF02288864">doi:10.1007/BF02288864</a>
</p>


<h3>See Also</h3>

<p>Other interaction tools: 
<code>probe_interaction()</code>,
<code>sim_margins()</code>,
<code>sim_slopes()</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R"># Using a fitted lm model
states &lt;- as.data.frame(state.x77)
states$HSGrad &lt;- states$`HS Grad`
fit &lt;- lm(Income ~ HSGrad + Murder*Illiteracy,
  data = states)
johnson_neyman(model = fit, pred = Murder,
  modx = Illiteracy)

</code></pre>


</div>