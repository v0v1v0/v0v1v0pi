<div class="container">

<table style="width: 100%;"><tr>
<td>irtQ-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>irtQ: Unidimensional Item Response Theory Modeling</h2>

<h3>Description</h3>

<p>Fit unidimensional item response theory (IRT) models to a mixture of dichotomous and polytomous data,
calibrate online item parameters (i.e., pretest and operational items), estimate examinees' abilities,
and  provide useful functions related to unidimensional IRT such as IRT model-data fit evaluation and
differential item functioning analysis.
</p>
<p>For the item parameter estimation, the marginal maximum likelihood estimation via the expectation-maximization (MMLE-EM) algorithm
(Bock &amp; Aitkin, 1981) is used. Also, the fixed item parameter calibration (FIPC) method (Kim, 2006) and
the fixed ability parameter calibration (FAPC) method, (Ban, Hanson, Wang, Yi, &amp; Harris, 2001; stocking, 1988),
often called Stocking's Method A, are provided. For the ability estimation, several popular scoring methods (e.g., ML, EAP, and MAP)
are implemented.
</p>
<p>In addition, there are many useful functions related to IRT analyses such as evaluating IRT model-data fit,
analyzing differential item functioning (DIF), importing item and/or ability parameters from popular IRT software,
running flexMIRT (Cai, 2017) through R, generating simulated data, computing the conditional distribution of observed scores
using the Lord-Wingersky recursion formula, computing item and test information functions, computing item and test characteristic
curve functions, and plotting item and test characteristic curves and item and test information functions.
</p>

<table>
<tr>
<td style="text-align: left;"> Package: </td>
<td style="text-align: left;"> irtQ</td>
</tr>
<tr>
<td style="text-align: left;"> Version: </td>
<td style="text-align: left;"> 0.2.1</td>
</tr>
<tr>
<td style="text-align: left;"> Date: </td>
<td style="text-align: left;">
2024-08-23</td>
</tr>
<tr>
<td style="text-align: left;"> Depends: </td>
<td style="text-align: left;"> R (&gt;= 4.1)</td>
</tr>
<tr>
<td style="text-align: left;"> License: </td>
<td style="text-align: left;"> GPL (&gt;= 2)</td>
</tr>
<tr>
<td style="text-align: left;"> </td>
</tr>
</table>
<h3>Details</h3>

<p>Following five sections describe a) how to implement the online item calibration using FIPC, a) how to implement the online item
calibration using Method A, b) two illustrations of the online calibration, and c) IRT Models used in <span class="pkg">irtQ</span> package.
</p>


<h3>Online item calibration with the fixed item parameter calibration method (e.g., Kim, 2006)</h3>

<p>The fixed item parameter calibration (FIPC) is one of useful online item calibration methods for computerized adaptive testing (CAT)
to put the parameter estimates of pretest items on the same scale of operational item parameter estimates without post hoc
linking/scaling (Ban, Hanson, Wang, Yi, &amp; Harris, 2001; Chen &amp; Wang, 2016). In FIPC, the operational item parameters are fixed to
estimate the characteristic of the underlying latent variable prior distribution when calibrating the pretest items. More specifically,
the underlying latent variable prior distribution of the operational items is estimated during the calibration of the pretest
items to put the item parameters of the pretest items on the scale of the operational item parameters (Kim, 2006). In the <span class="pkg">irtQ</span>
package, FIPC is implemented with two main steps:
</p>

<ol>
<li>
<p> Prepare a response data set and the item metadata of the fixed (or operational) items.
</p>
</li>
<li>
<p> Implement FIPC to estimate the item parameters of pretest items using the <code>est_irt</code> function.
</p>
</li>
</ol>
<dl>
<dt>1. Preparing a data set</dt>
<dd>
<p>To run the <code>est_irt</code> function, it requires two data sets:
</p>

<ol>
<li>
<p> Item metadata set (i.e., model, score category, and item parameters. see the desciption of the argument <code>x</code> in the function <code>est_irt</code>).
</p>
</li>
<li>
<p> Examinees' response data set for the items. It should be a matrix format where a row and column indicate the examinees and the items, respectively.
The order of the columns in the response data set must be exactly the same as the order of rows of the item metadata.
</p>
</li>
</ol>
</dd>
<dt>2. Estimating the pretest item parameters</dt>
<dd>
<p>When FIPC is implemented in <code>est_irt</code> function, the pretest item parameters are estimated by fixing the operational item parameters. To estimate the item
parameters, you need to provide the item metadata in the argument <code>x</code> and the response data in the argument <code>data</code>.
</p>
<p>It is worthwhile to explain about how to prepare the item metadata set in the argument <code>x</code>. A specific form of a data frame should be used for
the argument <code>x</code>. The first column should have item IDs, the second column should contain the number of score categories of the items, and the third
column should include IRT models. The available IRT models are "1PLM", "2PLM", "3PLM", and "DRM" for dichotomous items, and "GRM" and "GPCM" for polytomous
items. Note that "DRM" covers all dichotomous IRT models (i.e, "1PLM", "2PLM", and "3PLM") and "GRM" and "GPCM" represent the graded response model and
(generalized) partial credit model, respectively. From the fourth column, item parameters should be included. For dichotomous items, the fourth, fifth,
and sixth columns represent the item discrimination (or slope), item difficulty, and item guessing parameters, respectively. When "1PLM" or "2PLM" is
specified for any items in the third column, NAs should be inserted for the item guessing parameters. For polytomous items, the item discrimination (or slope)
parameters should be contained in the fourth column and the item threshold (or step) parameters should be included from the fifth to the last columns.
When the number of categories differs between items, the empty cells of item parameters should be filled with NAs. See 'est_irt' for more details about
the item metadata.
</p>
<p>Also, you should specify in the argument <code>fipc = TRUE</code> and a specific FIPC method in the argument <code>fipc.method</code>. Finally, you should provide
a vector of the location of the items to be fixed in the argument <code>fix.loc</code>. For more details about implementing FIPC, see the
description of the function <code>est_irt</code>.
</p>
<p>When implementing FIPC, you can estimate both the emprical histogram and the scale of latent variable prior distribution by setting <code>EmpHist = TRUE</code>.
If <code>EmpHist = FALSE</code>, the normal prior distribution is used during the item parameter estimation and the scale of the normal prior distribution is
updated during the EM cycle.
</p>
<p>The <code>est_item</code> function requires a vector of the number of score categories for the items in the argument <code>cats</code>. For example, a dichotomous item has
two score categories. If a single numeric value is specified, that value will be recycled across all items. If NULL and all items are binary items
(i.e., dichotomous items), it assumes that all items have two score categories.
</p>
<p>If necessary, you need to specify whether prior distributions of item slope and guessing parameters (only for the IRT 3PL model) are used in the arguments of
<code>use.aprior</code> and <code>use.gprior</code>, respectively. If you decide to use the prior distributions, you should specify what distributions will be used for the prior
distributions in the arguments of <code>aprior</code> and <code>gprior</code>, respectively. Currently three probability distributions of Beta, Log-normal, and Normal
distributions are available.
</p>
<p>In addition, if the response data include missing values, you must indicate the missing value in argument <code>missing</code>.
</p>
<p>Once the <code>est_irt</code> function has been implemented, you'll get a list of several internal objects such as the item parameter estimates,
standard error of the parameter estimates.
</p>
</dd>
</dl>
<h3>Online item calibration with the fixed ability parameter calibration method (e.g., Stocking, 1988)</h3>

<p>In CAT, the fixed ability parameter calibration (FAPC), often called Stocking's Method A, is the relatively simplest
and most straightforward online calibration method, which is the maximum likelihood estimation of the item parameters
given the proficiency estimates. In CAT, FAPC can be used to put the parameter estimates of pretest items on
the same scale of operational item parameter estimates and recalibrate the operational items to evaluate the parameter
drifts of the operational items (Chen &amp; Wang, 2016; Stocking, 1988). Also, FAPC is known to result in accurate, unbiased
item parameters calibration when items are randomly rather than adaptively administered to examinees, which occurs most
commonly with pretest items (Ban, Hanson, Wang, Yi, &amp; Harris, 2001; Chen &amp; Wang, 2016). Using <span class="pkg">irtQ</span> package,
the FAPC is implemented to calibrate the items with two main steps:
</p>

<ol>
<li>
<p> Prepare a data set for the calibration of item parameters (i.e., item response data and ability estimates).
</p>
</li>
<li>
<p> Implement the FAPC to estimate the item parameters using the <code>est_item</code> function.
</p>
</li>
</ol>
<dl>
<dt>1. Preparing a data set</dt>
<dd>
<p>To run the <code>est_item</code> function, it requires two data sets:
</p>

<ol>
<li>
<p> Examinees' ability (or proficiency) estimates. It should be in the format of a numeric vector.
</p>
</li>
<li>
<p> response data set for the items. It should be in the format of matrix where a row and column indicate
the examinees and the items, respectively. The order of the examinees in the response data set must be exactly the same as that of the examinees' ability estimates.
</p>
</li>
</ol>
</dd>
<dt>2. Estimating the pretest item parameters</dt>
<dd>
<p>The <code>est_item</code> function estimates the pretest item parameters given the proficiency estimates. To estimate the item parameters,
you need to provide the response data in the argument <code>data</code> and the ability estimates in the argument <code>score</code>.
</p>
<p>Also, you should provide a string vector of the IRT models in the argument <code>model</code> to indicate what IRT model is used to calibrate each item.
Available IRT models are "1PLM", "2PLM", "3PLM", and "DRM" for dichotomous items, and "GRM" and "GPCM" for polytomous items. "GRM" and "GPCM" represent
the graded response model and (generalized) partial credit model, respectively. Note that "DRM" is considered as "3PLM" in this function. If a single
character of the IRT model is specified, that model will be recycled across all items.
</p>
<p>The <code>est_item</code> function requires a vector of the number of score categories for the items in the argument <code>cats</code>. For example, a dichotomous item has
two score categories. If a single numeric value is specified, that value will be recycled across all items. If NULL and all items are binary items
(i.e., dichotomous items), it assumes that all items have two score categories.
</p>
<p>If necessary, you need to specify whether prior distributions of item slope and guessing parameters (only for the IRT 3PL model) are used in the arguments of
<code>use.aprior</code> and <code>use.gprior</code>, respectively. If you decide to use the prior distributions, you should specify what distributions will be used for the prior
distributions in the arguments of <code>aprior</code> and <code>gprior</code>, respectively. Currently three probability distributions of Beta, Log-normal, and Normal
distributions are available.
</p>
<p>In addition, if the response data include missing values, you must indicate the missing value in argument <code>missing</code>.
</p>
<p>Once the <code>est_item</code> function has been implemented, you'll get a list of several internal objects such as the item parameter estimates,
standard error of the parameter estimates.
</p>
</dd>
</dl>
<h3>Three examples of R script</h3>

<p>The example code below shows how to implement the online calibration and how to evaluate the IRT model-data fit:</p>
<pre>
##---------------------------------------------------------------
# Attach the packages
library(irtQ)

##----------------------------------------------------------------------------
# 1. The example code below shows how to prepare the data sets and how to
#    implement the fixed item parameter calibration (FIPC):
##----------------------------------------------------------------------------

## Step 1: prepare a data set
## In this example, we generated examinees' true proficiency parameters and simulated
## the item response data using the function "simdat".

## import the "-prm.txt" output file from flexMIRT
flex_sam &lt;- system.file("extdata", "flexmirt_sample-prm.txt", package = "irtQ")

# select the item metadata
x &lt;- bring.flexmirt(file=flex_sam, "par")$Group1$full_df

# generate 1,000 examinees' latent abilities from N(0.4, 1.3)
set.seed(20)
score &lt;- rnorm(1000, mean=0.4, sd=1.3)

# simulate the response data
sim.dat &lt;- simdat(x=x, theta=score, D=1)

## Step 2: Estimate the item parameters
# fit the 3PL model to all dichotmous items, fit the GRM model to all polytomous data,
# fix the five 3PL items (1st - 5th items) and three GRM items (53th to 55th items)
# also, estimate the empirical histogram of latent variable
fix.loc &lt;- c(1:5, 53:55)
(mod.fix1 &lt;- est_irt(x=x, data=sim.dat, D=1, use.gprior=TRUE,
                    gprior=list(dist="beta", params=c(5, 16)), EmpHist=TRUE, Etol=1e-3,
                    fipc=TRUE, fipc.method="MEM", fix.loc=fix.loc))
summary(mod.fix1)

# plot the estimated empirical histogram of latent variable prior distribution
(emphist &lt;- getirt(mod.fix1, what="weights"))
plot(emphist$weight ~ emphist$theta, xlab="Theta", ylab="Density")


##----------------------------------------------------------------------------
# 2. The example code below shows how to prepare the data sets and how to estimate
#    the item parameters using the fixed abilit parameter calibration (FAPC):
##----------------------------------------------------------------------------

## Step 1: prepare a data set
## In this example, we generated examinees' true proficiency parameters and simulated
## the item response data using the function "simdat". Because, the true
## proficiency parameters are not known in reality, however, the true proficiencies
## would be replaced with the proficiency estimates for the calibration.

# import the "-prm.txt" output file from flexMIRT
flex_sam &lt;- system.file("extdata", "flexmirt_sample-prm.txt", package = "irtQ")

# select the item metadata
x &lt;- bring.flexmirt(file=flex_sam, "par")$Group1$full_df

# modify the item metadata so that some items follow 1PLM, 2PLM and GPCM
x[c(1:3, 5), 3] &lt;- "1PLM"
x[c(1:3, 5), 4] &lt;- 1
x[c(1:3, 5), 6] &lt;- 0
x[c(4, 8:12), 3] &lt;- "2PLM"
x[c(4, 8:12), 6] &lt;- 0
x[54:55, 3] &lt;- "GPCM"

# generate examinees' abilities from N(0, 1)
set.seed(23)
score &lt;- rnorm(500, mean=0, sd=1)

# simulate the response data
data &lt;- simdat(x=x, theta=score, D=1)

## Step 2: Estimate the item parameters
# 1) item parameter estimation: constrain the slope parameters of the 1PLM to be equal
(mod1 &lt;- est_item(x, data, score, D=1, fix.a.1pl=FALSE, use.gprior=TRUE,
                 gprior=list(dist="beta", params=c(5, 17)), use.startval=FALSE))
summary(mod1)

# 2) item parameter estimation: fix the slope parameters of the 1PLM to 1
(mod2 &lt;- est_item(x, data, score, D=1, fix.a.1pl=TRUE, a.val.1pl=1, use.gprior=TRUE,
                 gprior=list(dist="beta", params=c(5, 17)), use.startval=FALSE))
summary(mod2)

# 3) item parameter estimation: fix the guessing parameters of the 3PLM to 0.2
(mod3 &lt;- est_item(x, data, score, D=1, fix.a.1pl=TRUE, fix.g=TRUE, a.val.1pl=1, g.val=.2,
                 use.startval=FALSE))
summary(mod3)


</pre>


<h3>IRT Models</h3>

<p>In the <span class="pkg">irtQ</span> package, both dichotomous and polytomous IRT models are available.
For dichotomous items, IRT one-, two-, and three-parameter logistic models (1PLM, 2PLM, and 3PLM) are used.
For polytomous items, the graded response model (GRM) and the (generalized) partial credit model (GPCM) are used.
Note that the item discrimination (or slope) parameters should be fixed to 1 when the partial credit model is fit to data.
</p>
<p>In the following, let <code class="reqn">Y</code> be the response of an examinee with latent ability <code class="reqn">\theta</code> on an item and suppose that there
are <code class="reqn">K</code> unique score categories for each polytomous item.
</p>

<dl>
<dt>IRT 1-3PL models</dt>
<dd>
<p>For the IRT 1-3PL models, the probability that an examinee with <code class="reqn">\theta</code> provides a correct answer for an item is given by,
</p>
<p style="text-align: center;"><code class="reqn">P(Y = 1|\theta) = g + \frac{(1 - g)}{1 + exp(-Da(\theta - b))},</code>
</p>

<p>where <code class="reqn">a</code> is the item discrimination (or slope) parameter, <code class="reqn">b</code> represents the item difficulty parameter,
<code class="reqn">g</code> refers to the item guessing parameter. <code class="reqn">D</code> is a scaling factor in IRT models to make the logistic function
as close as possible to the normal ogive function when <code class="reqn">D = 1.702</code>. When the 1PLM is used, <code class="reqn">a</code> is either fixed to a constant
value (e.g., <code class="reqn">a=1</code>) or constrained to have the same value across all 1PLM item data. When the IRT 1PLM or 2PLM is fit to data,
<code class="reqn">g = 0</code> is set to 0.
</p>
</dd>
<dt>GRM</dt>
<dd>
<p>For the GRM, the probability that an examinee with latent ability <code class="reqn">\theta</code> responds to score category <code class="reqn">k</code> (<code class="reqn">k=0,1,...,K-1</code>)
of an item is a given by,
</p>
<p style="text-align: center;"><code class="reqn">P(Y = k | \theta) = P^{*}(Y \ge k | \theta) - P^{*}(Y \ge k + 1 | \theta),</code>
</p>

<p style="text-align: center;"><code class="reqn">P^{*}(Y \ge k | \theta) = \frac{1}{1 + exp(-Da(\theta - b_{k}))}, and</code>
</p>

<p style="text-align: center;"><code class="reqn">P^{*}(Y \ge k + 1 | \theta) = \frac{1}{1 + exp(-Da(\theta - b_{k+1}))}, </code>
</p>

<p>where <code class="reqn">P^{*}(Y \ge k | \theta</code> refers to the category boundary (threshold) function for score category <code class="reqn">k</code> of an item
and its formula is analogous to that of 2PLM. <code class="reqn">b_{k}</code> is the difficulty (or threshold) parameter for category boundary
<code class="reqn">k</code> of an item. Note that <code class="reqn">P(Y = 0 | \theta) = 1 - P^{*}(Y \ge 1 | \theta)</code>
and <code class="reqn">P(Y = K-1 | \theta) = P^{*}(Y \ge K-1 | \theta)</code>.
</p>
</dd>
<dt>GPCM</dt>
<dd>
<p>For the GPCM, the probability that an examinee with latent ability <code class="reqn">\theta</code> responds to score category <code class="reqn">k</code> (<code class="reqn">k=0,1,...,K-1</code>)
of an item is a given by,
</p>
<p style="text-align: center;"><code class="reqn">P(Y = k | \theta) = \frac{exp(\sum_{v=0}^{k}{Da(\theta - b_{v})})}{\sum_{h=0}^{K-1}{exp(\sum_{v=0}^{h}{Da(\theta - b_{v})})}},</code>
</p>

<p>where <code class="reqn">b_{v}</code> is the difficulty parameter for category boundary <code class="reqn">v</code> of an item. In other contexts, the difficulty parameter <code class="reqn">b_{v}</code>
can also be parameterized as <code class="reqn">b_{v} = \beta - \tau_{v}</code>, where <code class="reqn">\beta</code> refers to the location (or overall difficulty) parameter
and <code class="reqn">\tau_{jv}</code> represents a threshold parameter for score category <code class="reqn">v</code> of an item. In the <span class="pkg">irtQ</span> package, <code class="reqn">K-1</code> difficulty
parameters are necessary when an item has <code class="reqn">K</code> unique score categories because <code class="reqn">b_{0}=0</code>. When a partial credit model is fit to data, <code class="reqn">a</code>
is fixed to 1.
</p>
</dd>
</dl>
<h3>Author(s)</h3>

<p>Hwanggyu Lim <a href="mailto:hglim83@gmail.com">hglim83@gmail.com</a>
</p>


<h3>References</h3>

<p>Ames, A. J., &amp; Penfield, R. D. (2015). An NCME Instructional Module on Item-Fit Statistics for Item Response Theory Models.
<em>Educational Measurement: Issues and Practice, 34</em>(3), 39-48.
</p>
<p>Baker, F. B., &amp; Kim, S. H. (2004). <em>Item response theory: Parameter estimation techniques.</em> CRC Press.
</p>
<p>Ban, J. C., Hanson, B. A., Wang, T., Yi, Q., &amp; Harris, D., J. (2001) A comparative study of on-line pretest item calibration/scaling methods
in computerized adaptive testing. <em>Journal of Educational Measurement, 38</em>(3), 191-212.
</p>
<p>Birnbaum, A. (1968). Some latent trait models and their use in inferring an examinee's ability. In F. M. Lord &amp; M. R. Novick (Eds.),
<em>Statistical theories of mental test scores</em> (pp. 397-479). Reading, MA: Addison-Wesley.
</p>
<p>Bock, R.D. (1960), <em>Methods and applications of optimal scaling</em>. Chapel Hill, NC: L.L. Thurstone Psychometric Laboratory.
</p>
<p>Bock, R. D., &amp; Aitkin, M. (1981). Marginal maximum likelihood estimation of item parameters: Application of an EM algorithm.
<em>Psychometrika, 46</em>, 443-459.
</p>
<p>Bock, R. D., &amp; Mislevy, R. J. (1982). Adaptive EAP estimation of ability in a microcomputer environment. <em>Psychometrika, 35</em>, 179-198.
</p>
<p>Cai, L. (2017). flexMIRT 3.5 Flexible multilevel multidimensional item analysis and test scoring [Computer software].
Chapel Hill, NC: Vector Psychometric Group.
</p>
<p>Cappaert, K. J., Wen, Y., &amp; Chang, Y. F. (2018). Evaluating CAT-adjusted
approaches for suspected item parameter drift detection. <em>Measurement:
Interdisciplinary Research and Perspectives, 16</em>(4), 226-238.
</p>
<p>Chalmers, R. P. (2012). mirt: A multidimensional item response theory package for the R environment.
<em>Journal of Statistical Software, 48</em>(6), 1-29.
</p>
<p>Chen, P., &amp; Wang, C. (2016). A new online calibration method for multidimensional computerized adaptive testing.
<em>Psychometrika, 81</em>(3), 674-701.
</p>
<p>Gonz√°lez, J. (2014). SNSequate: Standard and nonstandard statistical models and methods for test equating.
<em>Journal of Statistical Software, 59</em>, 1-30.
</p>
<p>Hambleton, R. K., &amp; Swaminathan, H. (1985) <em>Item response theory: Principles and applications</em>.
Boston, MA: Kluwer.
</p>
<p>Hambleton, R. K., Swaminathan, H., &amp; Rogers, H. J. (1991) <em>Fundamentals of item response theory</em>.
Newbury Park, CA: Sage.
</p>
<p>Han, K. T. (2016). Maximum likelihood score estimation method with fences for short-length tests and computerized adaptive tests.
<em>Applied psychological measurement, 40</em>(4), 289-301.
</p>
<p>Howard, J. P. (2017). <em>Computational methods for numerical analysis with R</em>. New York:
Chapman and Hall/CRC.
</p>
<p>Kang, T., &amp; Chen, T. T. (2008). Performance of the generalized S-X2 item fit index for polytomous IRT models.
<em>Journal of Educational Measurement, 45</em>(4), 391-406.
</p>
<p>Kim, S. (2006). A comparative study of IRT fixed parameter calibration methods.
<em>Journal of Educational Measurement, 43</em>(4), 355-381.
</p>
<p>Kolen, M. J. &amp; Brennan, R. L. (2004) <em>Test Equating, Scaling, and Linking</em> (2nd ed.). New York:
Springer.
</p>
<p>Kolen, M. J. &amp; Tong, Y. (2010). Psychometric properties of IRT proficiency estimates.
<em>Educational Measurement: Issues and Practice, 29</em>(3), 8-14.
</p>
<p>Laplace, P. S. (1820).<em>Theorie analytique des probabilites</em> (in French). Courcier.
</p>
<p>Li, Y. &amp; Lissitz, R. (2004). Applications of the analytically derived asymptotic standard errors of item response theory
item parameter estimates. <em>Journal of educational measurement, 41</em>(2), 85-117.
</p>
<p>Lim, H., &amp; Choe, E. M. (2023). Detecting differential item functioning in CAT using IRT residual DIF approach.
<em>Journal of Educational Measurement</em>. <a href="https://doi.org/10.1111/jedm.12366">doi:10.1111/jedm.12366</a>.
</p>
<p>Lim, H., Choe, E. M., &amp; Han, K. T. (2022). A residual-based differential item functioning detection framework in
item response theory. <em>Journal of Educational Measurement, 59</em>(1), 80-104. <a href="https://doi.org/10.1111/jedm.12313">doi:10.1111/jedm.12313</a>.
</p>
<p>Lim, H., Zhu, D., Choe, E. M., &amp; Han, K. T. (2023, April). <em>Detecting differential item functioning among multiple groups
using IRT residual DIF framework</em>. Paper presented at the Annual Meeting of the National Council on Measurement
in Education. Chicago, IL.
</p>
<p>Lim, H., Davey, T., &amp; Wells, C. S. (2020). A recursion-based analytical approach to evaluate the performance of MST.
<em>Journal of Educational Measurement, 58</em>(2), 154-178.
</p>
<p>Lord, F. &amp; Wingersky, M. (1984). Comparison of IRT true score and equipercentile observed score equatings.
<em>Applied Psychological Measurement, 8</em>(4), 453-461.
</p>
<p>Magis, D., &amp; Barrada, J. R. (2017). Computerized adaptive testing with R: Recent updates of the package catR.
<em>Journal of Statistical Software, 76</em>, 1-19.
</p>
<p>Magis, D., Yan, D., &amp; Von Davier, A. A. (2017). <em>Computerized adaptive
and multistage testing with R: Using packages catR and mstR</em>. Springer.
</p>
<p>McKinley, R., &amp; Mills, C. (1985). A comparison of several goodness-of-fit statistics.
<em>Applied Psychological Measurement, 9</em>, 49-57.
</p>
<p>Meilijson, I. (1989). A fast improvement to the EM algorithm on its own terms.
<em>Journal of the Royal Statistical Society: Series B (Methodological), 51</em>, 127-138.
</p>
<p>Muraki, E. &amp; Bock, R. D. (2003). PARSCALE 4: IRT item analysis and test scoring for rating
scale data [Computer Program]. Chicago, IL: Scientific Software International. URL http://www.ssicentral.com
</p>
<p>Newcombe, R. G. (1998). Two-sided confidence intervals for the single proportion: comparison of seven methods.
<em>Statistics in medicine, 17</em>(8), 857-872.
</p>
<p>Orlando, M., &amp; Thissen, D. (2000). Likelihood-based item-fit indices for dichotomous item response theory models.
<em>Applied Psychological Measurement, 24</em>(1), 50-64.
</p>
<p>Orlando, M., &amp; Thissen, D. (2003). Further investigation of the performance of S-X2: An item fit index for use with
dichotomous item response theory models. <em>Applied Psychological Measurement, 27</em>(4), 289-298.
</p>
<p>Pritikin, J. (2018). <em>rpf: Response Probability Functions</em>. R package version 0.59.
https://CRAN.R-project.org/package=rpf.
</p>
<p>Pritikin, J. N., &amp; Falk, C. F. (2020). OpenMx: A modular research environment for item response theory method development.
Applied Psychological Measurement, 44(7-8), 561-562.
</p>
<p>Stocking, M. L. (1996). An alternative method for scoring adaptive tests.
<em>Journal of Educational and Behavioral Statistics, 21</em>(4), 365-389.
</p>
<p>Stocking, M. L. (1988). <em>Scale drift in on-line calibration</em> (Research Rep. 88-28). Princeton, NJ: ETS.
</p>
<p>Stone, C. A. (2000). Monte Carlo based null distribution for an alternative
goodness-of-fit test statistic in IRT models. <em>Journal of educational
measurement, 37</em>(1), 58-75.
</p>
<p>Thissen, D. (1982). Marginal maximum likelihood estimation for the one-parameter logistic model.
<em>Psychometrika, 47</em>, 175-186.
</p>
<p>Thissen, D. &amp; Wainer, H. (1982). Weighted likelihood estimation of ability in item response theory.
<em>Psychometrika, 54</em>(3), 427-450.
</p>
<p>Thissen, D., Pommerich, M., Billeaud, K., &amp; Williams, V. S. (1995). Item Response Theory
for Scores on Tests Including Polytomous Items with Ordered Responses. <em>Applied Psychological
Measurement, 19</em>(1), 39-49.
</p>
<p>Thissen, D. &amp; Orlando, M. (2001). Item response theory for items scored in two categories. In D. Thissen &amp; H. Wainer (Eds.),
<em>Test scoring</em> (pp.73-140). Mahwah, NJ: Lawrence Erlbaum.
</p>
<p>Wainer, H., &amp; Mislevy, R. J. (1990). Item response theory, item calibration, and proficiency estimation. In H. Wainer (Ed.),
<em>Computer adaptive testing: A primer</em> (Chap. 4, pp.65-102). Hillsdale, NJ: Lawrence Erlbaum.
</p>
<p>Warm, T. A. (1989). Weighted likelihood estimation of ability in item response theory. <em>Psychometrika, 54</em>(3),
427-450.
</p>
<p>Weeks, J. P. (2010). plink: An R Package for Linking Mixed-Format Tests Using IRT-Based Methods.
<em>Journal of Statistical Software, 35</em>(12), 1-33. URL http://www.jstatsoft.org/v35/i12/.
</p>
<p>Wells, C. S., &amp; Bolt, D. M. (2008). Investigation of a nonparametric procedure for assessing goodness-of-fit in
item response theory. <em>Applied Measurement in Education, 21</em>(1), 22-40.
</p>
<p>Wilson, E. B. (1927). Probable inference, the law of succession, and statistical inference.
<em>Journal of the American Statistical Association, 22</em>(158), 209-212.
</p>
<p>Woods, C. M. (2007). Empirical histograms in item response theory with ordinal data. <em>Educational and Psychological Measurement, 67</em>(1), 73-87.
</p>
<p>Yen, W. M. (1981). Using simulation results to choose a latent trait model. <em>Applied Psychological Measurement, 5</em>, 245-262.
</p>
<p>Zimowski, M. F., Muraki, E., Mislevy, R. J., &amp; Bock, R. D. (2003). BILOG-MG 3: Multiple-group
IRT analysis and test maintenance for binary items [Computer Program]. Chicago, IL: Scientific
Software International. URL http://www.ssicentral.com
</p>


</div>