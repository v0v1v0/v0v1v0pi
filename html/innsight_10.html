<div class="container">

<table style="width: 100%;"><tr>
<td>DeepSHAP</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Deep Shapley additive explanations (DeepSHAP)</h2>

<h3>Description</h3>

<p>The <em>DeepSHAP</em> method extends the <code>DeepLift</code> technique by not only
considering a single reference value but by calculating the average
from several, ideally representative reference values at each layer. The
obtained feature-wise results are approximate Shapley values for the
chosen output, where the conditional expectation is computed using these
different reference values, i.e., the <em>DeepSHAP</em> method decompose the
difference from the prediction and the mean prediction <code class="reqn">f(x) - E[f(\tilde{x})]</code>
in feature-wise effects. The reference values can be passed by the argument
<code>data_ref</code>.
</p>
<p>The R6 class can also be initialized using the <code>run_deepshap</code> function
as a helper function so that no prior knowledge of R6 classes is required.
</p>


<h3>Super class</h3>

<p><code>innsight::InterpretingMethod</code> -&gt; <code>DeepSHAP</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>rule_name</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Name of the applied rule to calculate the contributions.
Either <code>'rescale'</code> or <code>'reveal_cancel'</code>.<br></p>
</dd>
<dt><code>data_ref</code></dt>
<dd>
<p>(<code>list</code>)<br>
The passed reference dataset for estimating the conditional expectation
as a <code>list</code> of <code>torch_tensors</code> in the selected
data format (field <code>dtype</code>) matching the corresponding shapes of the
individual input layers. Besides, the channel axis is moved to the
second position after the batch size because internally only the
format <em>channels first</em> is used.<br></p>
</dd>
</dl>
</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-DeepSHAP-new"><code>DeepSHAP$new()</code></a>
</p>
</li>
<li> <p><a href="#method-DeepSHAP-clone"><code>DeepSHAP$clone()</code></a>
</p>
</li>
</ul>
<details open><summary>Inherited methods</summary><ul>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="get_result"><a href="../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-get_result"><code>innsight::InterpretingMethod$get_result()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot"><a href="../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot"><code>innsight::InterpretingMethod$plot()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot_global"><a href="../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot_global"><code>innsight::InterpretingMethod$plot_global()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="print"><a href="../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-print"><code>innsight::InterpretingMethod$print()</code></a></span></li>
</ul></details><hr>
<a id="method-DeepSHAP-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>Create a new instance of the <code>DeepSHAP</code> R6 class. When initialized,
the method <em>DeepSHAP</em> is applied to the given data and the results are
stored in the field <code>result</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>DeepSHAP$new(
  converter,
  data,
  channels_first = TRUE,
  output_idx = NULL,
  output_label = NULL,
  ignore_last_act = TRUE,
  rule_name = "rescale",
  data_ref = NULL,
  limit_ref = 100,
  winner_takes_all = TRUE,
  verbose = interactive(),
  dtype = "float"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>converter</code></dt>
<dd>
<p>(<code>Converter</code>)<br>
An instance of the <code>Converter</code> class that includes the
torch-converted model and some other model-specific attributes. See
<code>Converter</code> for details.<br></p>
</dd>
<dt><code>data</code></dt>
<dd>
<p>(<code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or <code>list</code>)<br>
The data to which the method is to be applied. These must
have the same format as the input data of the passed model to the
converter object. This means either
</p>

<ul>
<li>
<p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>(batch_size, dim_in)</em>, if e.g., the model has only one input layer, or
</p>
</li>
<li>
<p> a <code>list</code> with the corresponding input data (according to the
upper point) for each of the input layers.<br></p>
</li>
</ul>
</dd>
<dt><code>channels_first</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
The channel position of the given data (argument
<code>data</code>). If <code>TRUE</code>, the channel axis is placed at the second position
between the batch size and the rest of the input axes, e.g.,
<code>c(10,3,32,32)</code> for a batch of ten images with three channels and a
height and width of 32 pixels. Otherwise (<code>FALSE</code>), the channel axis
is at the last position, i.e., <code>c(10,32,32,3)</code>. If the data
has no channel axis, use the default value <code>TRUE</code>.<br></p>
</dd>
<dt><code>output_idx</code></dt>
<dd>
<p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br>
These indices specify the output nodes for which
the method is to be applied. In order to allow models with multiple
output layers, there are the following possibilities to select
the indices of the output nodes in the individual output layers:
</p>

<ul>
<li>
<p> An <code>integer</code> vector of indices: If the model has only one output
layer, the values correspond to the indices of the output nodes, e.g.,
<code>c(1,3,4)</code> for the first, third and fourth output node. If there are
multiple output layers, the indices of the output nodes from the first
output layer are considered.
</p>
</li>
<li>
<p> A <code>list</code> of <code>integer</code> vectors of indices: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired indices of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of indices, e.g., <code>list(NULL, c(1,3))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br></p>
</li>
</ul>
</dd>
<dt><code>output_label</code></dt>
<dd>
<p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br>
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li>
<p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li>
<p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br></p>
</li>
</ul>
</dd>
<dt><code>ignore_last_act</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
Set this logical value to include the last
activation functions for each output layer, or not (default: <code>TRUE</code>).
In practice, the last activation (especially for softmax activation) is
often omitted.<br></p>
</dd>
<dt><code>rule_name</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Name of the applied rule to calculate the
contributions. Use either <code>'rescale'</code> or <code>'reveal_cancel'</code>. <br></p>
</dd>
<dt><code>data_ref</code></dt>
<dd>
<p>(<code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or <code>list</code>)<br>
The reference data which is used to estimate the conditional expectation.
These must have the same format as the input data of the passed model to
the converter object. This means either
</p>

<ul>
<li>
<p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>(batch_size, dim_in)</em>, if e.g., the model has only one input layer, or
</p>
</li>
<li>
<p> a <code>list</code> with the corresponding input data (according to the
upper point) for each of the input layers.
</p>
</li>
<li>
<p> or <code>NULL</code> (default) to use only a zero baseline for the estimation.<br></p>
</li>
</ul>
</dd>
<dt><code>limit_ref</code></dt>
<dd>
<p>(<code>integer(1)</code>)<br>
This argument limits the number of instances taken from the reference
dataset <code>data_ref</code> so that only random <code>limit_ref</code> elements and not
the entire dataset are used to estimate the conditional expectation.
A too-large number can significantly increase the computation time.<br></p>
</dd>
<dt><code>winner_takes_all</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
This logical argument is only relevant for MaxPooling
layers and is otherwise ignored. With this layer type, it is possible that
the position of the maximum values in the pooling kernel of the normal input
<code class="reqn">x</code> and the reference input <code class="reqn">x'</code> may not match, which leads to a
violation of the summation-to-delta property. To overcome this problem,
another variant is implemented, which treats a MaxPooling layer as an
AveragePooling layer in the backward pass only, leading to an uniform
distribution of the upper-layer contribution to the lower layer.<br></p>
</dd>
<dt><code>verbose</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
This logical argument determines whether a progress bar is
displayed for the calculation of the method or not. The default value is
the output of the primitive R function <code>interactive()</code>.<br></p>
</dd>
<dt><code>dtype</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
The data type for the calculations. Use
either <code>'float'</code> for torch_float or <code>'double'</code> for
torch_double.<br></p>
</dd>
</dl>
</div>


<hr>
<a id="method-DeepSHAP-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>DeepSHAP$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>References</h3>

<p>S. Lundberg &amp; S. Lee (2017) <em>A unified approach to interpreting model
predictions.</em>  NIPS 2017, p. 4768–4777
</p>


<h3>See Also</h3>

<p>Other methods: 
<code>ConnectionWeights</code>,
<code>DeepLift</code>,
<code>ExpectedGradient</code>,
<code>Gradient</code>,
<code>IntegratedGradient</code>,
<code>LIME</code>,
<code>LRP</code>,
<code>SHAP</code>,
<code>SmoothGrad</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
#----------------------- Example 1: Torch ----------------------------------
library(torch)

# Create nn_sequential model and data
model &lt;- nn_sequential(
  nn_linear(5, 12),
  nn_relu(),
  nn_linear(12, 2),
  nn_softmax(dim = 2)
)
data &lt;- torch_randn(25, 5)

# Create a reference dataset for the estimation of the conditional
# expectation
ref &lt;- torch_randn(5, 5)

# Create Converter
converter &lt;- convert(model, input_dim = c(5))

# Apply method DeepSHAP
deepshap &lt;- DeepSHAP$new(converter, data, data_ref = ref)

# You can also use the helper function `run_deepshap` for initializing
# an R6 DeepSHAP object
deepshap &lt;- run_deepshap(converter, data, data_ref = ref)

# Print the result as a torch tensor for first two data points
get_result(deepshap, "torch.tensor")[1:2]

# Plot the result for both classes
plot(deepshap, output_idx = 1:2)

# Plot the boxplot of all datapoints and for both classes
boxplot(deepshap, output_idx = 1:2)

# ------------------------- Example 2: Neuralnet ---------------------------
if (require("neuralnet")) {
  library(neuralnet)
  data(iris)

  # Train a neural network
  nn &lt;- neuralnet((Species == "setosa") ~ Petal.Length + Petal.Width,
    iris,
    linear.output = FALSE,
    hidden = c(3, 2), act.fct = "tanh", rep = 1
  )

  # Convert the model
  converter &lt;- convert(nn)

  # Apply DeepSHAP with rescale-rule and a 100 (default of `limit_ref`)
  # instances as the reference dataset
  deepshap &lt;- run_deepshap(converter, iris[, c(3, 4)],
                           data_ref = iris[, c(3, 4)])

  # Get the result as a dataframe and show first 5 rows
  get_result(deepshap, type = "data.frame")[1:5, ]

  # Plot the result for the first datapoint in the data
  plot(deepshap, data_idx = 1)

  # Plot the result as boxplots
  boxplot(deepshap)
}


# ------------------------- Example 3: Keras -------------------------------
if (require("keras") &amp; keras::is_keras_available()) {
  library(keras)

  # Make sure keras is installed properly
  is_keras_available()

  data &lt;- array(rnorm(10 * 32 * 32 * 3), dim = c(10, 32, 32, 3))

  model &lt;- keras_model_sequential()
  model %&gt;%
    layer_conv_2d(
      input_shape = c(32, 32, 3), kernel_size = 8, filters = 8,
      activation = "softplus", padding = "valid") %&gt;%
    layer_conv_2d(
      kernel_size = 8, filters = 4, activation = "tanh",
      padding = "same") %&gt;%
    layer_conv_2d(
      kernel_size = 4, filters = 2, activation = "relu",
      padding = "valid") %&gt;%
    layer_flatten() %&gt;%
    layer_dense(units = 64, activation = "relu") %&gt;%
    layer_dense(units = 16, activation = "relu") %&gt;%
    layer_dense(units = 2, activation = "softmax")

  # Convert the model
  converter &lt;- convert(model)

  # Apply the DeepSHAP method with zero baseline (wich is equivalent to
  # DeepLift with zero baseline)
  deepshap &lt;- run_deepshap(converter, data, channels_first = FALSE)

  # Plot the result for the first image and both classes
  plot(deepshap, output_idx = 1:2)

  # Plot the pixel-wise median of the results
  plot_global(deepshap, output_idx = 1)
}


#------------------------- Plotly plots ------------------------------------
if (require("plotly")) {
  # You can also create an interactive plot with plotly.
  # This is a suggested package, so make sure that it is installed
  library(plotly)
  boxplot(deepshap, as_plotly = TRUE)
}

</code></pre>


</div>