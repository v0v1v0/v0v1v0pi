<div class="container">

<table style="width: 100%;"><tr>
<td>inaparc-package</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Initialization Algorithms for Partitioning Cluster Analysis</h2>

<h3>Description</h3>

<p>Partitioning clustering algorithms divide data sets into k subsets or partitions which are so-called clusters. They require some initialization procedures for starting to partition the data sets. Initialization of cluster prototypes is one of such kind of procedures for most of the partitioning algorithms. Cluster prototypes are the data elements, i.e. centroids or medoids, representing the clusters in a data set. In order to initialize cluster prototypes, the package ‘<span class="pkg">inaparc</span>’ contains a set of the functions that are the implementations of widely-used algorithms in addition to some novel techniques. Initialization of fuzzy membership degrees matrices is another important task for starting the probabilistic and possibilistic partitioning algorithms. In order to initialize membership degrees matrices required by these algorithms, the package ‘<span class="pkg">inaparc</span>’ contains a number of functions for most of the data independent and dependent initialization techniques (Borgelt, 2005) which are categorized as the linear time-complexity and loglinear time complexity-initialization methods in Celebi et al (2013). 
</p>


<h3>Details</h3>

<p>Clustering is one of the most widely used exploratory statistical analysis in data mining. Its goal is to explore the groups of objects that are similar to each other within the group but different from the objects in other groups. According to a common taxonomy, the existing clustering algorithms are classified in two groups: Hierarchical and Non-hierarchical (or flat) algorithms (Rokah &amp; Maimon, 2005). As a dominant subfamily of non-hierarchical algorithms, the partitioning clustering algorithms divide data objects into a pre-defined number of clusters, which are the non-overlapping subsets of data. Although the choice of an appropriate algorithm for any clustering task depends on many criteria or purposes. When data size and dimensions are the concerned criteria, the non-hierarchical algorithms may be more practical way of clustering the large size and high dimensional data sets because they quickly process the large data sets when compared to the hierarchical clustering algorithms.
</p>
<p>As the most crowded group of the partitioning clustering tools, the prototype-based algorithms partition data objects into clusters in which each data object is more similar to its prototype than the prototypes of other clusters. On clustering context, a prototype is a typical data item that represents or characterizes a cluster (Tan et al. 2006). Usually, it can be regarded as the most central data point in a data subspace so-called cluster. The prototype of a cluster is so often a centroid, i.e., the mean of all the objects in a cluster. On the other hand, centroids can not be computed for non-numeric data, i.e., on nominal or ordinal data. In such case, medoids can be used as the prototypes of clusters (Tan et al, 2006). 
</p>
<p>Initialization or seeding is a process for selecting the starting values of cluster prototypes matrix which serves the initial representatives of clusters. It is an important task in partitioning cluster analysis because it is known that the final clustering result is to be highly sensitive to the initial prototypes of the clusters (Khan, 2012). When the prototypes are chosen to be equal or close to the actual centers of clusters in a data set, the partitioning converges quickly and yields quality results. Contrarily, poor initializations of prototype matrix may result with no-good quality of final partitions. 
</p>
<p>In fuzzy and possibilistic clustering, an object is a member of all clusters in varying degrees of membership instead of being a member of only one cluster. A membership degrees matrix is required by the fuzzy clustering algorithms, i.e., Fuzzy C-means (FCM) (Bezdek, 1981). Initialization of membership degrees for starting FCM and its various variants must satisfy the following constraints:  
</p>
<p style="text-align: center;"><code class="reqn">u_{ij}\in[0,1]; 1\le i \le n, 1\le j \le k</code>
</p>

<p style="text-align: center;"><code class="reqn">\sum\limits_{j=1}^k u_{ij}=1; 1\le i \le n</code>
</p>

<p style="text-align: center;"><code class="reqn">0&lt;\sum\limits_{i=1}^n u_{ij} &lt; n ; 1\le j \le k</code>
</p>

<p>Membership degrees matrices are usually initialized with the techniques based on random number generating as the function <code>imembrand</code> does. In addition to these common techiques, a novel technique using the information from synthetically produced classes over a selected feature is provided in the package ‘<span class="pkg">inaparc</span>’. The novel technique which is implemented in <code>figen</code> may contribute to the fast convergence of the clustering algorithms when compared to the random sampling based techniques. The package also serves the functions for building hard or crisp membership degrees which can be used for testing purposes.
</p>


<h3>Author(s)</h3>

<p>Zeynel Cebeci, Cagatay Cebeci</p>


<h3>References</h3>

<p>Bezdek J.C. (1981). Pattern recognition with fuzzy objective function algorithms. Plenum, NY, 256 p. &lt;ISBN:0306406713&gt;
</p>
<p>Borgelt, C., (2005). <em>Prototype-based classification and clustering</em>. Habilitationsschrift zur Erlangung der Venia legendi fuer Informatik, vorgelegt der Fakultaet fuer Informatik der Otto-von-Guericke-Universitaet Magdeburg, Magdeburg, 22 June 2005. url:<a href="https://borgelt.net/habil/pbcc.pdf">https://borgelt.net/habil/pbcc.pdf</a>
</p>
<p>Cebeci, Z. (2018). Initialization of Membership Degree Matrix for Fast Convergence of Fuzzy C-Means Clustering", In Proc. of <em>2018 International Conference on Artificial Intelligence and Data Processing (IDAP)</em>, IEEE, Sep. 2018, pp. 1-5., <a href="https://doi.org/10.1109/IDAP.2018.8620920">doi:10.1109/IDAP.2018.8620920</a>
</p>
<p>Cebeci, Z., Sahin, M. &amp; Cebeci, C. (2018). Data dependent techniques for initialization of cluster prototypes in partitioning cluster analysis. In Proc. of <em>4th International Conference on Engineering and Natural Science</em>, Kiev, Ukraine, May 2018. pp. 12-22. 
</p>
<p>Rokah, L. &amp; Maimon, O. (2005). Clustering methods. In <em>Data Mining and Knowledge Discovery Handbook (ed. O. Maimon)</em>, Springer US. pp. 321-352. <a href="https://doi.org/10.1.1.149.9326">doi:10.1.1.149.9326</a>
</p>
<p>Tan, P. N., Steinbach, M., &amp; Kumar, V. (2006). Cluster analysis: Basic concepts and algorithms. In <em>Introduction to Data Mining</em>. Pearson Addison Wesley. url:<a href="https://www-users.cse.umn.edu/~kumar/dmbook/ch8.pdf">https://www-users.cse.umn.edu/~kumar/dmbook/ch8.pdf</a>
</p>
<p>Khan, F. (2012). An initial seed selection algorithm for k-means clustering of georeferenced data to improve replicability of cluster assignments for mapping application. <em>Applied Soft Computing</em>, 12 (11) : 3698-3700. <a href="https://doi.org/10.1016/j.asoc.2012.07.021">doi:10.1016/j.asoc.2012.07.021</a>
</p>
<p>Celebi, M.E., Kingravi, H.A. &amp; Vela, P.A. (2013). A comparative study of efficient initialization methods for the K-means clustering algorithm, <em>Expert Systems with Applications</em>, 40 (1): 200-210. arXiv:<a href="https://arxiv.org/pdf/1209.1960.pdf">https://arxiv.org/pdf/1209.1960.pdf</a>
</p>


<h3>See Also</h3>

<p><code>aldaoud</code>,
<code>ballhall</code>,
<code>crsamp</code>,
<code>firstk</code>,
<code>forgy</code>,
<code>hartiganwong</code>,
<code>imembones</code>,
<code>imembrand</code>,
<code>figen</code>,
<code>inofrep</code>,
<code>inscsf</code>,
<code>insdev</code>,
<code>is.inaparc</code>,
<code>kkz</code>,
<code>kmpp</code>,
<code>ksegments</code>,
<code>ksteps</code>,
<code>lastk</code>,
<code>lhsmaximin</code>,
<code>lhsrandom</code>,
<code>maximin</code>,
<code>mscseek</code>,
<code>rsamp</code>,
<code>rsegment</code>,
<code>scseek</code>,
<code>scseek2</code>,
<code>spaeth</code>,
<code>ssamp</code>,
<code>topbottom</code>,
<code>uniquek</code>,
<code>ursamp</code>
</p>


</div>