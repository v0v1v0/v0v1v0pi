<div class="container">

<table style="width: 100%;"><tr>
<td>LRP</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Layer-wise relevance propagation (LRP)</h2>

<h3>Description</h3>

<p>This is an implementation of the <em>layer-wise relevance propagation
(LRP)</em> algorithm introduced by Bach et al. (2015). It's a local method for
interpreting a single element of the dataset and calculates the relevance
scores for each input feature to the model output. The basic idea of this
method is to decompose the prediction score of the model with respect to
the input features, i.e.,
</p>
<p style="text-align: center;"><code class="reqn">f(x) = \sum_i R(x_i).</code>
</p>

<p>Because of the bias vector that absorbs some relevance, this decomposition
is generally an approximation. There exist several propagation rules to
determine the relevance scores. In this package are implemented: simple
rule ("simple"), <code class="reqn">\varepsilon</code>-rule ("epsilon") and
<code class="reqn">\alpha</code>-<code class="reqn">\beta</code>-rule ("alpha_beta").
</p>
<p>The R6 class can also be initialized using the <code>run_lrp</code> function
as a helper function so that no prior knowledge of R6 classes is required.
</p>


<h3>Super class</h3>

<p><code>innsight::InterpretingMethod</code> -&gt; <code>LRP</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>rule_name</code></dt>
<dd>
<p>(<code>character(1)</code> or <code>list</code>)<br>
The name of the rule with which the relevance scores
are calculated. Implemented are <code>"simple"</code>, <code>"epsilon"</code>, <code>"alpha_beta"</code>
(and <code>"pass"</code> but only for 'BatchNorm_Layer'). However, this value
can also be a named list that assigns one of these three rules to each
implemented layer type separately, e.g.,
<code>list(Dense_Layer = "simple", Conv2D_Layer = "alpha_beta")</code>.
Layers not specified in this list then use the default value <code>"simple"</code>.
The implemented layer types are:
</p>

<table>
<tr>
<td style="text-align: left;">
<code class="reqn">\cdot</code> 'Dense_Layer' </td>
<td style="text-align: left;"> <code class="reqn">\cdot</code> 'Conv1D_Layer' </td>
<td style="text-align: left;"> <code class="reqn">\cdot</code> 'Conv2D_Layer'</td>
</tr>
<tr>
<td style="text-align: left;">
<code class="reqn">\cdot</code> 'BatchNorm_Layer' </td>
<td style="text-align: left;"> <code class="reqn">\cdot</code> 'AvgPool1D_Layer' </td>
<td style="text-align: left;"> <code class="reqn">\cdot</code> 'AvgPool2D_Layer'</td>
</tr>
<tr>
<td style="text-align: left;">
<code class="reqn">\cdot</code> 'MaxPool1D_Layer' </td>
<td style="text-align: left;"> <code class="reqn">\cdot</code> 'MaxPool2D_Layer' </td>
<td style="text-align: left;">
</td>
</tr>
</table>
</dd>
<dt><code>rule_param</code></dt>
<dd>
<p>(<code>numeric</code> or <code>list</code>)<br>
The parameter of the selected rule. Similar to the
argument <code>rule_name</code>, this can also be a named list that assigns a
rule parameter to each layer type.<br></p>
</dd>
</dl>
</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-LRP-new"><code>LRP$new()</code></a>
</p>
</li>
<li> <p><a href="#method-LRP-clone"><code>LRP$clone()</code></a>
</p>
</li>
</ul>
<details open><summary>Inherited methods</summary><ul>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="get_result"><a href="../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-get_result"><code>innsight::InterpretingMethod$get_result()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot"><a href="../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot"><code>innsight::InterpretingMethod$plot()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot_global"><a href="../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot_global"><code>innsight::InterpretingMethod$plot_global()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="print"><a href="../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-print"><code>innsight::InterpretingMethod$print()</code></a></span></li>
</ul></details><hr>
<a id="method-LRP-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>Create a new instance of the <code>LRP</code> R6 class. When initialized,
the method <em>LRP</em> is applied to the given data and the results are stored in
the field <code>result</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>LRP$new(
  converter,
  data,
  channels_first = TRUE,
  output_idx = NULL,
  output_label = NULL,
  ignore_last_act = TRUE,
  rule_name = "simple",
  rule_param = NULL,
  winner_takes_all = TRUE,
  verbose = interactive(),
  dtype = "float"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>converter</code></dt>
<dd>
<p>(<code>Converter</code>)<br>
An instance of the <code>Converter</code> class that includes the
torch-converted model and some other model-specific attributes. See
<code>Converter</code> for details.<br></p>
</dd>
<dt><code>data</code></dt>
<dd>
<p>(<code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or <code>list</code>)<br>
The data to which the method is to be applied. These must
have the same format as the input data of the passed model to the
converter object. This means either
</p>

<ul>
<li>
<p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>(batch_size, dim_in)</em>, if e.g., the model has only one input layer, or
</p>
</li>
<li>
<p> a <code>list</code> with the corresponding input data (according to the
upper point) for each of the input layers.<br></p>
</li>
</ul>
</dd>
<dt><code>channels_first</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
The channel position of the given data (argument
<code>data</code>). If <code>TRUE</code>, the channel axis is placed at the second position
between the batch size and the rest of the input axes, e.g.,
<code>c(10,3,32,32)</code> for a batch of ten images with three channels and a
height and width of 32 pixels. Otherwise (<code>FALSE</code>), the channel axis
is at the last position, i.e., <code>c(10,32,32,3)</code>. If the data
has no channel axis, use the default value <code>TRUE</code>.<br></p>
</dd>
<dt><code>output_idx</code></dt>
<dd>
<p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br>
These indices specify the output nodes for which
the method is to be applied. In order to allow models with multiple
output layers, there are the following possibilities to select
the indices of the output nodes in the individual output layers:
</p>

<ul>
<li>
<p> An <code>integer</code> vector of indices: If the model has only one output
layer, the values correspond to the indices of the output nodes, e.g.,
<code>c(1,3,4)</code> for the first, third and fourth output node. If there are
multiple output layers, the indices of the output nodes from the first
output layer are considered.
</p>
</li>
<li>
<p> A <code>list</code> of <code>integer</code> vectors of indices: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired indices of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of indices, e.g., <code>list(NULL, c(1,3))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br></p>
</li>
</ul>
</dd>
<dt><code>output_label</code></dt>
<dd>
<p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br>
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li>
<p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li>
<p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br></p>
</li>
</ul>
</dd>
<dt><code>ignore_last_act</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
Set this logical value to include the last
activation functions for each output layer, or not (default: <code>TRUE</code>).
In practice, the last activation (especially for softmax activation) is
often omitted.<br></p>
</dd>
<dt><code>rule_name</code></dt>
<dd>
<p>(<code>character(1)</code> or <code>list</code>)<br>
The name of the rule with which the relevance scores
are calculated. Implemented are <code>"simple"</code>, <code>"epsilon"</code>, <code>"alpha_beta"</code>.
You can pass one of the above characters to apply this rule to all
possible layers. However, this value can also be a named list that
assigns one of these three rules to each
implemented layer type separately, e.g.,
<code>list(Dense_Layer = "simple", Conv2D_Layer = "alpha_beta")</code>.
Layers not specified in this list then use the default value <code>"simple"</code>.
The implemented layer types are:<br></p>

<table>
<tr>
<td style="text-align: left;">
<code class="reqn">\cdot</code> 'Dense_Layer' </td>
<td style="text-align: left;"> <code class="reqn">\cdot</code> 'Conv1D_Layer' </td>
<td style="text-align: left;"> <code class="reqn">\cdot</code> 'Conv2D_Layer'</td>
</tr>
<tr>
<td style="text-align: left;">
<code class="reqn">\cdot</code> 'BatchNorm_Layer' </td>
<td style="text-align: left;"> <code class="reqn">\cdot</code> 'AvgPool1D_Layer' </td>
<td style="text-align: left;"> <code class="reqn">\cdot</code> 'AvgPool2D_Layer'</td>
</tr>
<tr>
<td style="text-align: left;">
<code class="reqn">\cdot</code> 'MaxPool1D_Layer' </td>
<td style="text-align: left;"> <code class="reqn">\cdot</code> 'MaxPool2D_Layer' </td>
<td style="text-align: left;">
</td>
</tr>
</table>
<p><em>Note:</em> For normalization layers like 'BatchNorm_Layer', the rule
<code>"pass"</code> is implemented as well, which ignores such layers in the
backward pass.<br></p>
</dd>
<dt><code>rule_param</code></dt>
<dd>
<p>(<code>numeric(1)</code> or <code>list</code>)<br>
The parameter of the selected rule. Note: Only the
rules <code>"epsilon"</code> and <code>"alpha_beta"</code> take use of the
parameter. Use the default value <code>NULL</code> for the default parameters
("epsilon" : <code class="reqn">0.01</code>, "alpha_beta" : <code class="reqn">0.5</code>). Similar to the
argument <code>rule_name</code>, this can also be a named list that assigns a
rule parameter to each layer type. If the layer type is not specified
in the named list, the default parameters will be used.<br></p>
</dd>
<dt><code>winner_takes_all</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
This logical argument is only relevant for
models with a MaxPooling layer. Since many zeros are produced during
the backward pass due to the selection of the maximum value in the
pooling kernel, another variant is implemented, which treats a
MaxPooling as an AveragePooling layer in the backward pass to overcome
the problem of too many zero relevances. With the default value <code>TRUE</code>,
the whole upper-layer relevance is passed to the maximum value in each
pooling window. Otherwise, if <code>FALSE</code>, the relevance is distributed equally
among all nodes in a pooling window.<br></p>
</dd>
<dt><code>verbose</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
This logical argument determines whether a progress bar is
displayed for the calculation of the method or not. The default value is
the output of the primitive R function <code>interactive()</code>.<br></p>
</dd>
<dt><code>dtype</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
The data type for the calculations. Use
either <code>'float'</code> for torch_float or <code>'double'</code> for
torch_double.<br></p>
</dd>
</dl>
</div>



<h5>Returns</h5>

<p>A new instance of the R6 class <code>LRP</code>.
</p>


<hr>
<a id="method-LRP-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>LRP$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>References</h3>

<p>S. Bach et al. (2015) <em>On pixel-wise explanations for non-linear
classifier decisions by layer-wise relevance propagation.</em> PLoS ONE 10,
p. 1-46
</p>


<h3>See Also</h3>

<p>Other methods: 
<code>ConnectionWeights</code>,
<code>DeepLift</code>,
<code>DeepSHAP</code>,
<code>ExpectedGradient</code>,
<code>Gradient</code>,
<code>IntegratedGradient</code>,
<code>LIME</code>,
<code>SHAP</code>,
<code>SmoothGrad</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
 #----------------------- Example 1: Torch ----------------------------------
library(torch)

# Create nn_sequential model and data
model &lt;- nn_sequential(
  nn_linear(5, 12),
  nn_relu(),
  nn_linear(12, 2),
  nn_softmax(dim = 2)
)
data &lt;- torch_randn(25, 5)

# Create Converter
converter &lt;- convert(model, input_dim = c(5))

# Apply method LRP with simple rule (default)
lrp &lt;- LRP$new(converter, data)

# You can also use the helper function `run_lrp` for initializing
# an R6 LRP object
lrp &lt;- run_lrp(converter, data)

# Print the result as an array for data point one and two
get_result(lrp)[1:2,,]

# Plot the result for both classes
plot(lrp, output_idx = 1:2)

# Plot the boxplot of all datapoints without a preprocess function
boxplot(lrp, output_idx = 1:2, preprocess_FUN = identity)

# ------------------------- Example 2: Neuralnet ---------------------------
if (require("neuralnet")) {
  library(neuralnet)
  data(iris)
  nn &lt;- neuralnet(Species ~ .,
    iris,
    linear.output = FALSE,
    hidden = c(10, 8), act.fct = "tanh", rep = 1, threshold = 0.5
  )

  # Create an converter for this model
  converter &lt;- convert(nn)

  # Create new instance of 'LRP'
  lrp &lt;- run_lrp(converter, iris[, -5], rule_name = "simple")

  # Get the result as an array for data point one and two
  get_result(lrp)[1:2,,]

  # Get the result as a torch tensor for data point one and two
  get_result(lrp, type = "torch.tensor")[1:2]

  # Use the alpha-beta rule with alpha = 2
  lrp &lt;- run_lrp(converter, iris[, -5],
    rule_name = "alpha_beta",
    rule_param = 2
  )

  # Include the last activation into the calculation
  lrp &lt;- run_lrp(converter, iris[, -5],
    rule_name = "alpha_beta",
    rule_param = 2,
    ignore_last_act = FALSE
  )

  # Plot the result for all classes
  plot(lrp, output_idx = 1:3)
}


# ------------------------- Example 3: Keras -------------------------------
if (require("keras") &amp; keras::is_keras_available()) {
  library(keras)

  # Make sure keras is installed properly
  is_keras_available()

  data &lt;- array(rnorm(10 * 60 * 3), dim = c(10, 60, 3))

  model &lt;- keras_model_sequential()
  model %&gt;%
    layer_conv_1d(
      input_shape = c(60, 3), kernel_size = 8, filters = 8,
      activation = "softplus", padding = "valid") %&gt;%
    layer_conv_1d(
      kernel_size = 8, filters = 4, activation = "tanh",
      padding = "same") %&gt;%
    layer_conv_1d(
      kernel_size = 4, filters = 2, activation = "relu",
      padding = "valid") %&gt;%
    layer_flatten() %&gt;%
    layer_dense(units = 64, activation = "relu") %&gt;%
    layer_dense(units = 16, activation = "relu") %&gt;%
    layer_dense(units = 3, activation = "softmax")

  # Convert the model
  converter &lt;- convert(model)

  # Apply the LRP method with the epsilon rule for the dense layers and
  # the alpha-beta rule for the convolutional layers
  lrp_comp &lt;- run_lrp(converter, data,
    channels_first = FALSE,
    rule_name = list(Dense_Layer = "epsilon", Conv1D_Layer = "alpha_beta"),
    rule_param = list(Dense_Layer = 0.1, Conv1D_Layer = 1)
  )

  # Plot the result for the first datapoint and all classes
  plot(lrp_comp, output_idx = 1:3)

  # Plot the result as boxplots for first two classes
  boxplot(lrp_comp, output_idx = 1:2)
}


#------------------------- Plotly plots ------------------------------------
if (require("plotly")) {
  # You can also create an interactive plot with plotly.
  # This is a suggested package, so make sure that it is installed
  library(plotly)

  # Result as boxplots
  boxplot(lrp, as_plotly = TRUE)

  # Result of the second data point
  plot(lrp, data_idx = 2, as_plotly = TRUE)
}

</code></pre>


</div>