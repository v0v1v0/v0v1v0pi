<div class="container">

<table style="width: 100%;"><tr>
<td>ConnectionWeights</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Connection weights method</h2>

<h3>Description</h3>

<p>This class implements the <em>Connection weights</em> method investigated by
Olden et al. (2004), which results in a relevance score for each input
variable. The basic idea is to multiply all path weights for each
possible connection between an input feature and the output node and then
calculate the sum over them. Besides, it is originally a global
interpretation method and independent of the input data. For a neural
network with <code class="reqn">3</code> hidden layers with weight matrices <code class="reqn">W_1</code>,
<code class="reqn">W_2</code> and <code class="reqn">W_3</code>, this method results in a simple matrix
multiplication independent of the activation functions in between:
</p>
<p style="text-align: center;"><code class="reqn">W_1 * W_2 * W_3.</code>
</p>

<p>In this package, we extended this method to a local method inspired by the
method <em>Gradient<code class="reqn">\times</code>Input</em> (see <code>Gradient</code>). Hence, the local variant is
simply the point-wise product of the global <em>Connection weights</em> method and
the input data. You can use this variant by setting the <code>times_input</code>
argument to <code>TRUE</code> and providing input data.
</p>
<p>The R6 class can also be initialized using the <code>run_cw</code> function
as a helper function so that no prior knowledge of R6 classes is required.
</p>


<h3>Super class</h3>

<p><code>innsight::InterpretingMethod</code> -&gt; <code>ConnectionWeights</code>
</p>


<h3>Public fields</h3>

<div class="r6-fields">

<dl>
<dt><code>times_input</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
This logical value indicates whether the results from
the <em>Connection weights</em> method were multiplied by the provided input
data or not. Thus, this value specifies whether the original global
variant of the method or the local one was applied. If the value is
<code>TRUE</code>, then data is provided in the field <code>data</code>.</p>
</dd>
</dl>
</div>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-ConnectionWeights-new"><code>ConnectionWeights$new()</code></a>
</p>
</li>
<li> <p><a href="#method-ConnectionWeights-clone"><code>ConnectionWeights$clone()</code></a>
</p>
</li>
</ul>
<details open><summary>Inherited methods</summary><ul>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="get_result"><a href="../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-get_result"><code>innsight::InterpretingMethod$get_result()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot"><a href="../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot"><code>innsight::InterpretingMethod$plot()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot_global"><a href="../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot_global"><code>innsight::InterpretingMethod$plot_global()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="print"><a href="../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-print"><code>innsight::InterpretingMethod$print()</code></a></span></li>
</ul></details><hr>
<a id="method-ConnectionWeights-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>Create a new instance of the class <code>ConnectionWeights</code>. When
initialized, the method is applied and the results
are stored in the field <code>result</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>ConnectionWeights$new(
  converter,
  data = NULL,
  output_idx = NULL,
  output_label = NULL,
  channels_first = TRUE,
  times_input = FALSE,
  verbose = interactive(),
  dtype = "float"
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>converter</code></dt>
<dd>
<p>(<code>Converter</code>)<br>
An instance of the <code>Converter</code> class that includes the
torch-converted model and some other model-specific attributes. See
<code>Converter</code> for details.<br></p>
</dd>
<dt><code>data</code></dt>
<dd>
<p>(<code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or <code>list</code>)<br>
The data to which the method is to be applied. These must
have the same format as the input data of the passed model to the
converter object. This means either
</p>

<ul>
<li>
<p> an <code>array</code>, <code>data.frame</code>, <code>torch_tensor</code> or array-like format of
size <em>(batch_size, dim_in)</em>, if e.g.the model has only one input layer, or
</p>
</li>
<li>
<p> a <code>list</code> with the corresponding input data (according to the
upper point) for each of the input layers.
</p>
</li>
</ul>
<p>This argument is only relevant if
<code>times_input</code> is <code>TRUE</code>, otherwise it will be ignored because it is a
locale (i.e. explanation for each data point individually) method only
in this case.<br></p>
</dd>
<dt><code>output_idx</code></dt>
<dd>
<p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br>
These indices specify the output nodes for which
the method is to be applied. In order to allow models with multiple
output layers, there are the following possibilities to select
the indices of the output nodes in the individual output layers:
</p>

<ul>
<li>
<p> An <code>integer</code> vector of indices: If the model has only one output
layer, the values correspond to the indices of the output nodes, e.g.,
<code>c(1,3,4)</code> for the first, third and fourth output node. If there are
multiple output layers, the indices of the output nodes from the first
output layer are considered.
</p>
</li>
<li>
<p> A <code>list</code> of <code>integer</code> vectors of indices: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired indices of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of indices, e.g., <code>list(NULL, c(1,3))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br></p>
</li>
</ul>
</dd>
<dt><code>output_label</code></dt>
<dd>
<p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br>
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li>
<p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li>
<p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br></p>
</li>
</ul>
</dd>
<dt><code>channels_first</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
The channel position of the given data (argument
<code>data</code>). If <code>TRUE</code>, the channel axis is placed at the second position
between the batch size and the rest of the input axes, e.g.,
<code>c(10,3,32,32)</code> for a batch of ten images with three channels and a
height and width of 32 pixels. Otherwise (<code>FALSE</code>), the channel axis
is at the last position, i.e., <code>c(10,32,32,3)</code>. If the data
has no channel axis, use the default value <code>TRUE</code>.<br></p>
</dd>
<dt><code>times_input</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
Multiplies the results with the input features.
This variant turns the global <em>Connection weights</em> method into a local
one. Default: <code>FALSE</code>.<br></p>
</dd>
<dt><code>verbose</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
This logical argument determines whether a progress bar is
displayed for the calculation of the method or not. The default value is
the output of the primitive R function <code>interactive()</code>.<br></p>
</dd>
<dt><code>dtype</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
The data type for the calculations. Use
either <code>'float'</code> for torch_float or <code>'double'</code> for
torch_double.<br></p>
</dd>
</dl>
</div>


<hr>
<a id="method-ConnectionWeights-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>ConnectionWeights$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>References</h3>


<ul><li>
<p> J. D. Olden et al. (2004) <em>An accurate comparison of methods for
quantifying variable importance in artificial neural networks using
simulated data.</em> Ecological Modelling 178, p. 389–397
</p>
</li></ul>
<h3>See Also</h3>

<p>Other methods: 
<code>DeepLift</code>,
<code>DeepSHAP</code>,
<code>ExpectedGradient</code>,
<code>Gradient</code>,
<code>IntegratedGradient</code>,
<code>LIME</code>,
<code>LRP</code>,
<code>SHAP</code>,
<code>SmoothGrad</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
#----------------------- Example 1: Torch ----------------------------------
library(torch)

# Create nn_sequential model
model &lt;- nn_sequential(
  nn_linear(5, 12),
  nn_relu(),
  nn_linear(12, 1),
  nn_sigmoid()
)

# Create Converter with input names
converter &lt;- Converter$new(model,
  input_dim = c(5),
  input_names = list(c("Car", "Cat", "Dog", "Plane", "Horse"))
)

# You can also use the helper function for the initialization part
converter &lt;- convert(model,
  input_dim = c(5),
  input_names = list(c("Car", "Cat", "Dog", "Plane", "Horse"))
)

# Apply method Connection Weights
cw &lt;- ConnectionWeights$new(converter)

# Again, you can use a helper function `run_cw()` for initializing
cw &lt;- run_cw(converter)

# Print the head of the result as a data.frame
head(get_result(cw, "data.frame"), 5)

# Plot the result
plot(cw)

#----------------------- Example 2: Neuralnet ------------------------------
if (require("neuralnet")) {
  library(neuralnet)
  data(iris)

  # Train a Neural Network
  nn &lt;- neuralnet((Species == "setosa") ~ Petal.Length + Petal.Width,
    iris,
    linear.output = FALSE,
    hidden = c(3, 2), act.fct = "tanh", rep = 1
  )

  # Convert the trained model
  converter &lt;- convert(nn)

  # Apply the Connection Weights method
  cw &lt;- run_cw(converter)

  # Get the result as a torch tensor
  get_result(cw, type = "torch.tensor")

  # Plot the result
  plot(cw)
}


# ------------------------- Example 3: Keras -------------------------------
if (require("keras") &amp; keras::is_keras_available()) {
  library(keras)

  # Make sure keras is installed properly
  is_keras_available()

  data &lt;- array(rnorm(10 * 32 * 32 * 3), dim = c(10, 32, 32, 3))

  model &lt;- keras_model_sequential()
  model %&gt;%
    layer_conv_2d(
      input_shape = c(32, 32, 3), kernel_size = 8, filters = 8,
      activation = "softplus", padding = "valid") %&gt;%
    layer_conv_2d(
      kernel_size = 8, filters = 4, activation = "tanh",
      padding = "same") %&gt;%
    layer_conv_2d(
      kernel_size = 4, filters = 2, activation = "relu",
      padding = "valid") %&gt;%
    layer_flatten() %&gt;%
    layer_dense(units = 64, activation = "relu") %&gt;%
    layer_dense(units = 16, activation = "relu") %&gt;%
    layer_dense(units = 2, activation = "softmax")

  # Convert the model
  converter &lt;- convert(model)

  # Apply the Connection Weights method
  cw &lt;- run_cw(converter)

  # Get the head of the result as a data.frame
  head(get_result(cw, type = "data.frame"), 5)

  # Plot the result for all classes
  plot(cw, output_idx = 1:2)
}


#------------------------- Plotly plots ------------------------------------
if (require("plotly")) {
  # You can also create an interactive plot with plotly.
  # This is a suggested package, so make sure that it is installed
  library(plotly)
  plot(cw, as_plotly = TRUE)
}

</code></pre>


</div>