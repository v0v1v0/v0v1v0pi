<div class="container">

<table style="width: 100%;"><tr>
<td>icaimax</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
ICA via Infomax Algorithm
</h2>

<h3>Description</h3>

<p>Computes ICA decomposition using Bell and Sejnowski's (1995) Information-Maximization (Infomax) approach with various options.
</p>


<h3>Usage</h3>

<pre><code class="language-R">icaimax(X, nc, center = TRUE, maxit = 100, tol = 1e-6, Rmat = diag(nc), 
        alg = "newton", fun = "tanh", signs = rep(1, nc), signswitch = TRUE, 
        rate = 1, rateanneal = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>X</code></td>
<td>

<p>Data matrix with <code>n</code> rows (samples) and <code>p</code> columns (variables).
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>nc</code></td>
<td>

<p>Number of components to extract.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>center</code></td>
<td>

<p>If <code>TRUE</code>, columns of <code>X</code> are mean-centered before ICA decomposition.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>maxit</code></td>
<td>

<p>Maximum number of algorithm iterations to allow.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>tol</code></td>
<td>

<p>Convergence tolerance.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Rmat</code></td>
<td>

<p>Initial estimate of the <code>nc</code>-by-<code>nc</code> orthogonal rotation matrix.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alg</code></td>
<td>

<p>Algorithm to use: <code>alg="newton"</code> for Newton iteration, and <code>alg="gradient"</code> for gradient descent.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fun</code></td>
<td>

<p>Nonlinear (squashing) function to use for algorithm: <code>fun="tanh"</code> for hyperbolic tangent, <code>fun="log"</code> for logistic, and <code>fun="ext"</code> for extended Infomax.  
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>signs</code></td>
<td>

<p>Vector of length <code>nc</code> such that <code>signs[j] = 1</code> if j-th component is super-Gaussian and <code>signs[j] = -1</code> if j-th component is sub-Gaussian. Only used if <code>fun="ext"</code>. Ignored if <code>signswitch=TRUE</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>signswitch</code></td>
<td>

<p>If <code>TRUE</code>, the <code>signs</code> vector is automatically determined from the data; otherwise a confirmatory ICA decomposition is calculated using input <code>signs</code> vector. Only used if <code>fun="ext"</code>.
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rate</code></td>
<td>

<p>Learing rate for gradient descent algorithm. Ignored if <code>alg="newton"</code>.  
</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rateanneal</code></td>
<td>

<p>Annealing angle and proportion for gradient descent learing rate (see Details). Ignored if <code>alg="newton"</code>.  
</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p><b>ICA Model</b>
The ICA model can be written as <code>X = tcrossprod(S, M) + E</code>, where <code>S</code> contains the source signals, <code>M</code> is the mixing matrix, and <code>E</code> contains the noise signals. Columns of <code>X</code> are assumed to have zero mean. The goal is to find the unmixing matrix <code>W</code> such that columns of <code>S = tcrossprod(X, W)</code> are independent as possible.
</p>
<p><b>Whitening</b>
Without loss of generality, we can write <code>M = P %*% R</code> where <code>P</code> is a tall matrix and <code>R</code> is an orthogonal rotation matrix. Letting <code>Q</code> denote the pseudoinverse of <code>P</code>, we can whiten the data using <code>Y = tcrossprod(X, Q)</code>. The goal is to find the orthongal rotation matrix <code>R</code> such that the source signal estimates <code>S = Y %*% R</code> are as independent as possible. Note that <code>W = crossprod(R, Q)</code>.
</p>
<p><b>Infomax</b>
The Infomax approach finds the orthogonal rotation matrix <code>R</code> that (approximately) maximizes the joint entropy of a nonlinear function of the estimated source signals. See Bell and Sejnowski (1995) and Helwig and Hong (2013) for specifics of algorithms.
</p>


<h3>Value</h3>

<table>
<tr style="vertical-align: top;">
<td><code>S</code></td>
<td>
<p>Matrix of source signal estimates (<code>S = Y %*% R</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>M</code></td>
<td>
<p>Estimated mixing matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>W</code></td>
<td>
<p>Estimated unmixing matrix (<code>W = crossprod(R, Q)</code>).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Y</code></td>
<td>
<p>Whitened data matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>Q</code></td>
<td>
<p>Whitening matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>R</code></td>
<td>
<p>Orthogonal rotation matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>vafs</code></td>
<td>
<p>Variance-accounted-for by each component.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>iter</code></td>
<td>
<p>Number of algorithm iterations.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>alg</code></td>
<td>
<p>Algorithm used (same as input).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>fun</code></td>
<td>
<p>Contrast function (same as input).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>signs</code></td>
<td>
<p>Component signs (same as input).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>rate</code></td>
<td>
<p>Learning rate (same as input).</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>converged</code></td>
<td>
<p>Logical indicating if algorithm converged.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Nathaniel E. Helwig &lt;helwig@umn.edu&gt;
</p>


<h3>References</h3>

<p>Bell, A.J. &amp; Sejnowski, T.J. (1995). An information-maximization approach to blind separation and blind deconvolution. <em>Neural Computation, 7</em>(6), 1129-1159. <a href="https://doi.org/10.1162/neco.1995.7.6.1129">doi:10.1162/neco.1995.7.6.1129</a>
</p>
<p>Helwig, N.E. &amp; Hong, S. (2013). A critique of Tensor Probabilistic Independent Component Analysis: Implications and recommendations for multi-subject fMRI data analysis. <em>Journal of Neuroscience Methods, 213</em>(2), 263-273. <a href="https://doi.org/10.1016/j.jneumeth.2012.12.009">doi:10.1016/j.jneumeth.2012.12.009</a>
</p>


<h3>See Also</h3>

<p><code>icafast</code> for FastICA
</p>
<p><code>icajade</code> for ICA via JADE
</p>


<h3>Examples</h3>

<pre><code class="language-R">##########   EXAMPLE 1   ##########

# generate noiseless data (p == r)
set.seed(123)
nobs &lt;- 1000
Amat &lt;- cbind(icasamp("a", "rnd", nobs), icasamp("b", "rnd", nobs))
Bmat &lt;- matrix(2 * runif(4), nrow = 2, ncol = 2)
Xmat &lt;- tcrossprod(Amat, Bmat)

# ICA via Infomax with 2 components
imod &lt;- icaimax(Xmat, nc = 2)
acy(Bmat, imod$M)
cor(Amat, imod$S)



##########   EXAMPLE 2   ##########

# generate noiseless data (p != r)
set.seed(123)
nobs &lt;- 1000
Amat &lt;- cbind(icasamp("a", "rnd", nobs), icasamp("b", "rnd", nobs))
Bmat &lt;- matrix(2 * runif(200), nrow = 100, ncol = 2)
Xmat &lt;- tcrossprod(Amat, Bmat)

# ICA via Infomax with 2 components
imod &lt;- icaimax(Xmat, nc = 2)
cor(Amat, imod$S)



##########   EXAMPLE 3   ##########

# generate noisy data (p != r)
set.seed(123)
nobs &lt;- 1000
Amat &lt;- cbind(icasamp("a", "rnd", nobs), icasamp("b", "rnd", nobs))
Bmat &lt;- matrix(2 * runif(200), 100, 2)
Emat &lt;- matrix(rnorm(10^5), nrow = 1000, ncol = 100)
Xmat &lt;- tcrossprod(Amat,Bmat) + Emat

# ICA via Infomax with 2 components
imod &lt;- icaimax(Xmat, nc = 2)
cor(Amat, imod$S)

</code></pre>


</div>