<div class="container">

<table style="width: 100%;"><tr>
<td>LIME</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Local interpretable model-agnostic explanations (LIME)</h2>

<h3>Description</h3>

<p>The R6 class <code>LIME</code> calculates the feature weights of a linear surrogate of
the prediction model for a instance to be explained, namely the
<em>local interpretable model-agnostic explanations (LIME)</em>. It is a
model-agnostic method that can be applied to any predictive model.
This means, in particular, that
<code>LIME</code> can be applied not only to objects of the <code>Converter</code> class but
also to any other model. The only requirement is the argument <code>pred_fun</code>,
which generates predictions with the model for given data. However, this
function is pre-implemented for models created with
<code>nn_sequential</code>, <code>keras_model</code>,
<code>neuralnet</code> or <code>Converter</code>. Internally, the
suggested package <code>lime</code> is utilized and applied to <code>data.frame</code>.
</p>
<p>The R6 class can also be initialized using the <code>run_lime</code> function
as a helper function so that no prior knowledge of R6 classes is required.
</p>
<p><strong>Note:</strong> Even signal and image data are initially transformed into a
<code>data.frame</code> using <code>as.data.frame()</code> and then <code>lime::lime</code> and
<code>lime::explain</code> are
applied. In other words, a custom <code>pred_fun</code> may need to convert the
<code>data.frame</code> back into an <code>array</code> as necessary.
</p>


<h3>Super classes</h3>

<p><code>innsight::InterpretingMethod</code> -&gt; <code>innsight::AgnosticWrapper</code> -&gt; <code>LIME</code>
</p>


<h3>Methods</h3>



<h4>Public methods</h4>


<ul>
<li> <p><a href="#method-LIME-new"><code>LIME$new()</code></a>
</p>
</li>
<li> <p><a href="#method-LIME-clone"><code>LIME$clone()</code></a>
</p>
</li>
</ul>
<details open><summary>Inherited methods</summary><ul>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="get_result"><a href="../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-get_result"><code>innsight::InterpretingMethod$get_result()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot"><a href="../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot"><code>innsight::InterpretingMethod$plot()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="plot_global"><a href="../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-plot_global"><code>innsight::InterpretingMethod$plot_global()</code></a></span></li>
<li><span class="pkg-link" data-pkg="innsight" data-topic="InterpretingMethod" data-id="print"><a href="../../innsight/html/InterpretingMethod.html#method-InterpretingMethod-print"><code>innsight::InterpretingMethod$print()</code></a></span></li>
</ul></details><hr>
<a id="method-LIME-new"></a>



<h4>Method <code>new()</code>
</h4>

<p>Create a new instance of the <code>LIME</code> R6 class. When initialized,
the method <em>LIME</em> is applied to the given data and the results are
stored in the field <code>result</code>.
</p>


<h5>Usage</h5>

<div class="r"><pre>LIME$new(
  model,
  data,
  data_ref,
  output_type = NULL,
  pred_fun = NULL,
  output_idx = NULL,
  output_label = NULL,
  channels_first = TRUE,
  input_dim = NULL,
  input_names = NULL,
  output_names = NULL,
  ...
)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>model</code></dt>
<dd>
<p>(any prediction model)<br>
A fitted model for a classification or regression task that
is intended to be interpreted. A <code>Converter</code> object can also be
passed. In order for the package to know how to make predictions
with the given model, a prediction function must also be passed with
the argument <code>pred_fun</code>. However, for models created by
<code>nn_sequential</code>, <code>keras_model</code>,
<code>neuralnet</code> or <code>Converter</code>,
these have already been pre-implemented and do not need to be
specified.<br></p>
</dd>
<dt><code>data</code></dt>
<dd>
<p>(<code>array</code>, <code>data.frame</code> or <code>torch_tensor</code>)<br>
The individual instances to be explained by the method.
These must have the same format as the input data of the passed model
and has to be either <code>matrix</code>, an <code>array</code>, a <code>data.frame</code> or a
<code>torch_tensor</code>. If no value is specified, all instances in the
dataset <code>data</code> will be explained.<br><strong>Note:</strong> For the model-agnostic methods, only models with a single
input and output layer is allowed!<br></p>
</dd>
<dt><code>data_ref</code></dt>
<dd>
<p>(<code>array</code>, <code>data.frame</code> or <code>torch_tensor</code>)<br>
The dataset to which the method is to be applied. These must
have the same format as the input data of the passed model and has to
be either <code>matrix</code>, an <code>array</code>, a <code>data.frame</code> or a
<code>torch_tensor</code>.<br><strong>Note:</strong> For the model-agnostic methods, only models with a single
input and output layer is allowed!<br></p>
</dd>
<dt><code>output_type</code></dt>
<dd>
<p>(<code>character(1)</code>)<br>
Type of the model output, i.e., either
<code>"classification"</code> or <code>"regression"</code>.<br></p>
</dd>
<dt><code>pred_fun</code></dt>
<dd>
<p>(<code>function</code>)<br>
Prediction function for the model. This argument is only
needed if <code>model</code> is not a model created by
<code>nn_sequential</code>, <code>keras_model</code>,
<code>neuralnet</code> or <code>Converter</code>. The first argument of
<code>pred_fun</code> has to be <code>newdata</code>, e.g.,
</p>
<div class="sourceCode"><pre>function(newdata, ...) model(newdata)
</pre></div>
</dd>
<dt><code>output_idx</code></dt>
<dd>
<p>(<code>integer</code>, <code>list</code> or <code>NULL</code>)<br>
These indices specify the output nodes for which
the method is to be applied. In order to allow models with multiple
output layers, there are the following possibilities to select
the indices of the output nodes in the individual output layers:
</p>

<ul>
<li>
<p> An <code>integer</code> vector of indices: If the model has only one output
layer, the values correspond to the indices of the output nodes, e.g.,
<code>c(1,3,4)</code> for the first, third and fourth output node. If there are
multiple output layers, the indices of the output nodes from the first
output layer are considered.
</p>
</li>
<li>
<p> A <code>list</code> of <code>integer</code> vectors of indices: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired indices of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of indices, e.g., <code>list(NULL, c(1,3))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br></p>
</li>
</ul>
</dd>
<dt><code>output_label</code></dt>
<dd>
<p>(<code>character</code>, <code>factor</code>, <code>list</code> or <code>NULL</code>)<br>
These values specify the output nodes for which
the method is to be applied. Only values that were previously passed with
the argument <code>output_names</code> in the <code>converter</code> can be used. In order to
allow models with multiple
output layers, there are the following possibilities to select
the names of the output nodes in the individual output layers:
</p>

<ul>
<li>
<p> A <code>character</code> vector or <code>factor</code> of labels: If the model has only one output
layer, the values correspond to the labels of the output nodes named in the
passed <code>Converter</code> object, e.g.,
<code>c("a", "c", "d")</code> for the first, third and fourth output node if the
output names are <code>c("a", "b", "c", "d")</code>. If there are
multiple output layers, the names of the output nodes from the first
output layer are considered.
</p>
</li>
<li>
<p> A <code>list</code> of <code>charactor</code>/<code>factor</code> vectors of labels: If the method is to be
applied to output nodes from different layers, a list can be passed
that specifies the desired labels of the output nodes for each
output layer. Unwanted output layers have the entry <code>NULL</code> instead of
a vector of labels, e.g., <code>list(NULL, c("a", "c"))</code> for the first and
third output node in the second output layer.
</p>
</li>
<li> <p><code>NULL</code> (default): The method is applied to all output nodes in
the first output layer but is limited to the first ten as the
calculations become more computationally expensive for more output
nodes.<br></p>
</li>
</ul>
</dd>
<dt><code>channels_first</code></dt>
<dd>
<p>(<code>logical(1)</code>)<br>
The channel position of the given data (argument
<code>data</code>). If <code>TRUE</code>, the channel axis is placed at the second position
between the batch size and the rest of the input axes, e.g.,
<code>c(10,3,32,32)</code> for a batch of ten images with three channels and a
height and width of 32 pixels. Otherwise (<code>FALSE</code>), the channel axis
is at the last position, i.e., <code>c(10,32,32,3)</code>. If the data
has no channel axis, use the default value <code>TRUE</code>.<br></p>
</dd>
<dt><code>input_dim</code></dt>
<dd>
<p>(<code>integer</code>)<br>
The model input dimension excluding the batch
dimension. It can be specified as vector of integers, but has to be in
the format "channels first".<br></p>
</dd>
<dt><code>input_names</code></dt>
<dd>
<p>(<code>character</code>, <code>factor</code> or <code>list</code>)<br>
The input names of the model excluding the batch dimension. For a model
with a single input layer and input axis (e.g., for tabular data), the
input names can be specified as a character vector or factor, e.g.,
for a dense layer with 3 input features use <code>c("X1", "X2", "X3")</code>. If
the model input consists of multiple axes (e.g., for signal and
image data), use a list of character vectors or factors for each axis
in the format "channels first", e.g., use
<code>list(c("C1", "C2"), c("L1","L2","L3","L4","L5"))</code> for a 1D
convolutional input layer with signal length 4 and 2 channels.<br><em>Note:</em> This argument is optional and otherwise the names are
generated automatically. But if this argument is set, all found
input names in the passed model will be disregarded.<br></p>
</dd>
<dt><code>output_names</code></dt>
<dd>
<p>(<code>character</code>, <code>factor</code> )<br>
A character vector with the names for the output dimensions
excluding the batch dimension, e.g., for a model with 3 output nodes use
<code>c("Y1", "Y2", "Y3")</code>. Instead of a character
vector you can also use a factor to set an order for the plots.<br><em>Note:</em> This argument is optional and otherwise the names are
generated automatically. But if this argument is set, all found
output names in the passed model will be disregarded.<br></p>
</dd>
<dt><code>...</code></dt>
<dd>
<p>other arguments forwarded to <code>lime::explain</code>.</p>
</dd>
</dl>
</div>


<hr>
<a id="method-LIME-clone"></a>



<h4>Method <code>clone()</code>
</h4>

<p>The objects of this class are cloneable with this method.
</p>


<h5>Usage</h5>

<div class="r"><pre>LIME$clone(deep = FALSE)</pre></div>



<h5>Arguments</h5>

<div class="arguments">

<dl>
<dt><code>deep</code></dt>
<dd>
<p>Whether to make a deep clone.</p>
</dd>
</dl>
</div>




<h3>See Also</h3>

<p>Other methods: 
<code>ConnectionWeights</code>,
<code>DeepLift</code>,
<code>DeepSHAP</code>,
<code>ExpectedGradient</code>,
<code>Gradient</code>,
<code>IntegratedGradient</code>,
<code>LRP</code>,
<code>SHAP</code>,
<code>SmoothGrad</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">
#----------------------- Example 1: Torch -----------------------------------
library(torch)

# Create nn_sequential model and data
model &lt;- nn_sequential(
  nn_linear(5, 12),
  nn_relu(),
  nn_linear(12, 2),
  nn_softmax(dim = 2)
  )
data &lt;- torch_randn(25, 5)

# Calculate LIME for the first 10 instances and set the
# feature and outcome names
lime &lt;- LIME$new(model, data[1:10, ], data_ref = data,
                 input_names = c("Car", "Cat", "Dog", "Plane", "Horse"),
                 output_names = c("Buy it!", "Don't buy it!"))

# You can also use the helper function `run_lime` for initializing
# an R6 LIME object
lime &lt;- run_lime(model, data[1:10, ], data_ref = data,
                 input_names = c("Car", "Cat", "Dog", "Plane", "Horse"),
                 output_names = c("Buy it!", "Don't buy it!"))

# Get the result as an array for the first two instances
get_result(lime)[1:2,, ]

# Plot the result for both classes
plot(lime, output_idx = c(1, 2))

# Show the boxplot over all 10 instances
boxplot(lime, output_idx = c(1, 2))

# We can also forward some arguments to lime::explain, e.g. n_permutatuins
# to get more accurate values
lime &lt;- run_lime(model, data[1:10, ], data_ref = data,
                 input_names = c("Car", "Cat", "Dog", "Plane", "Horse"),
                 output_names = c("Buy it!", "Don't buy it!"),
                 n_perturbations = 200)

# Plot the boxplots again
boxplot(lime, output_idx = c(1, 2))

#----------------------- Example 2: Converter object --------------------------
# We can do the same with an Converter object (all feature and outcome names
# will be extracted by the LIME method!)
conv &lt;- convert(model,
                input_dim = c(5),
                input_names = c("Car", "Cat", "Dog", "Plane", "Horse"),
                output_names = c("Buy it!", "Don't buy it!"))

# Calculate LIME for the first 10 instances
lime &lt;- run_lime(conv, data[1:10], data_ref = data, n_perturbations = 300)

# Plot the result for both classes
plot(lime, output_idx = c(1, 2))

#----------------------- Example 3: Other model -------------------------------
if (require("neuralnet") &amp; require("ranger")) {
  library(neuralnet)
  library(ranger)
  data(iris)

  # Fit a random forest unsing the ranger package
  model &lt;- ranger(Species ~ ., data = iris, probability = TRUE)

  # There is no pre-implemented predict function for ranger models, i.e.,
  # we have to define it ourselves.
  pred_fun &lt;- function(newdata, ...) {
    predict(model, newdata, ...)$predictions
  }

  # Calculate LIME for the instances of index 1 and 111 and add
  # the outcome labels (for LIME, the output_type is required!)
  lime &lt;- run_lime(model, iris[c(1, 111), -5],
                   data_ref = iris[, -5],
                   pred_fun = pred_fun,
                   output_type = "classification",
                   output_names = levels(iris$Species),
                   n_perturbations = 300)

  # Plot the result for the first two classes and all selected instances
  plot(lime, data_idx = 1:2, output_idx = 1:2)

  # Get the result as a torch_tensor
  get_result(lime, "torch_tensor")
}

</code></pre>


</div>