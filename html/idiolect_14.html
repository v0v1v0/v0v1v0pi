<div class="container">

<table style="width: 100%;"><tr>
<td>performance</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>Performance evaluation</h2>

<h3>Description</h3>

<p>This function is used to the test the performance of an authorship analysis method.
</p>


<h3>Usage</h3>

<pre><code class="language-R">performance(training, test = NULL)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>training</code></td>
<td>
<p>The data frame with the results to evaluate, typically the output of an authorship analysis function, such as <code>impostors()</code>. If only training is present then the function will perform a leave-one-out cross-validation.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>test</code></td>
<td>
<p>Optional data frame of results. If present then a calibration model is extracted from training and its performance is evaluated on this data set.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>Before applying a method to a real authorship case, it is good practice to test it on known ground truth data. This function performs this test by taking as input either a single table of results or two tables, one for training and one for the test, and then returning as output a list with the following performance statistics: the log-likelihood ratio cost (both <code class="reqn">C_{llr}</code> and <code class="reqn">C_{llr}^{min}</code>), Equal Error Rate (ERR), the mean values of the log-likelihood ratio for both the same-author (TRUE) and different-author (FALSE) cases, the Area Under the Curve (AUC), Balanced Accuracy, Precision, Recall, F1, and the full confusion matrix. The binary classification statistics are all calculated considering a Log-Likelihood Ratio score of 0 as a threshold.
</p>


<h3>Value</h3>

<p>The function returns a list containing a data frame with performance statistics, including an object that can be used to make a tippet plot using the <code>tippet.plot()</code> function of the <code>ROC</code> package (https://github.com/davidavdav/ROC).
</p>


<h3>Examples</h3>

<pre><code class="language-R">results &lt;- data.frame(score = c(0.5, 0.2, 0.8, 0.01), target = c(TRUE, FALSE, TRUE, FALSE))
perf &lt;- performance(results)
perf$evaluation

</code></pre>


</div>