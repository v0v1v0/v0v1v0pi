<div class="container">

<table style="width: 100%;"><tr>
<td>maximin</td>
<td style="text-align: right;">R Documentation</td>
</tr></table>
<h2>
Initialization of cluster prototypes using Maximin algorithm
</h2>

<h3>Description</h3>

<p>Initializes the cluster prototypes matrix by using the Maximin algorithm.
</p>


<h3>Usage</h3>

<pre><code class="language-R">maximin(x, k)
</code></pre>


<h3>Arguments</h3>

<table>
<tr style="vertical-align: top;">
<td><code>x</code></td>
<td>
<p>a numeric vector, data frame or matrix.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>k</code></td>
<td>
<p>an integer for the number of clusters.</p>
</td>
</tr>
</table>
<h3>Details</h3>

<p>The main idea of the <dfn>Maximin</dfn> algorithm is to isolate the cluster prototypes that are farthest apart (Philpot, 2001). The algorithm randomly samples one data object from the data set and assigns it as the first cluster prototype. The prototype of second cluster is determined as the data object which is farthest from the first prototype. Then, the remaining part of data set is scanned for the data objects whose distances are minimum to the previously selected prototypes. The object having the maximum of minimum distances is assigned the prototype of third cluster. The same procedure is repeated for determining the prototypes of other clusters (Spaeth, 1997; Gonzales, 1985; Duda et al, 2000, Celebi et al, 2013).
</p>
<p>The algorithm generally works well with circular shaped clusters whose radius are smaller than the separation between clusters.  However, it is very sensitive to the order of object in data sets. Also it is computationally expensive because each time once a new cluster prototype is selected, the distances must be computed for every object from every cluster prototype (Philpot, 2001). In order to contribute to the solutions of this problem, the current implementation of <code>maximin</code> includes a simple control that if an object has the minimum distance of zero, the seeking procedure is no more continued to compute the distances for the remaining objects. This control may speed the algorithm up with the <code>maximin</code> function in this package.  
</p>


<h3>Value</h3>

<p>an object of class ‘inaparc’, which is a list consists of the following items:
</p>
<table>
<tr style="vertical-align: top;">
<td><code>v</code></td>
<td>
<p>a numeric matrix of the initial cluster prototypes.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>ctype</code></td>
<td>
<p>a string representing the type of centroid, which used to build prototype matrix. Its value is ‘obj’ with this function because the cluster prototype matrix contains the objects.</p>
</td>
</tr>
<tr style="vertical-align: top;">
<td><code>call</code></td>
<td>
<p>a string containing the matched function call that generates the object ‘inaparc’.</p>
</td>
</tr>
</table>
<h3>Author(s)</h3>

<p>Zeynel Cebeci, Cagatay Cebeci
</p>


<h3>References</h3>

<p>Spaeth, H. (1977). Computational experiences with the exchange method: Applied to four commonly used partitioning cluster analysis criteria, <em>European Journal of Operational Research</em> 1 (1): 23-31. <a href="https://doi.org/10.1016/S0377-2217%2877%2981005-9">doi:10.1016/S0377-2217(77)81005-9</a>
</p>
<p>Gonzalez, T. (1985), Clustering to minimize the maximum intercluster distance, <em>Theoretical Computer Science</em> 38 (2-3): 293-306. url:<a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.366.8183&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.366.8183&amp;rep=rep1&amp;type=pdf</a>
</p>
<p>Duda, R.O., Hart, P.E. &amp; Stork, D.G. (2000). <em>Pattern Classification</em>, Wiley-Interscience. &lt;ISBN:978-0-471-05669-0&gt;
</p>
<p>Celebi, M.E., Kingravi, H.A. &amp; Vela, P.A. (2013). A comparative study of efficient initialization methods for the K-means clustering algorithm, <em>Expert Systems with Applications</em>, 40 (1): 200-210. arXiv:<a href="https://arxiv.org/pdf/1209.1960.pdf">https://arxiv.org/pdf/1209.1960.pdf</a>
</p>
<p>Philpot, W. (2001). Topic 8: Clustering/Unsupervised Classification in <em>Lecture Notes, CEE 615: Digital Image Processing - Jan 2001, Cornell Univ.</em>, url:<a href="https://www-users.cse.umn.edu/~kumar/dmbook/ch8.pdf">https://www-users.cse.umn.edu/~kumar/dmbook/ch8.pdf</a>
</p>


<h3>See Also</h3>

<p><code>aldaoud</code>,
<code>ballhall</code>,
<code>crsamp</code>,
<code>firstk</code>,
<code>forgy</code>,
<code>hartiganwong</code>,
<code>inofrep</code>,
<code>inscsf</code>,
<code>insdev</code>,
<code>kkz</code>,
<code>kmpp</code>,
<code>ksegments</code>,
<code>ksteps</code>,
<code>lastk</code>,
<code>lhsmaximin</code>,
<code>lhsrandom</code>,
<code>mscseek</code>,
<code>rsamp</code>,
<code>rsegment</code>,
<code>scseek</code>,
<code>scseek2</code>,
<code>spaeth</code>,
<code>ssamp</code>,
<code>topbottom</code>,
<code>uniquek</code>,
<code>ursamp</code>
</p>


<h3>Examples</h3>

<pre><code class="language-R">data(iris)
res &lt;-maximin(x=iris[,1:4], k=5)
v &lt;- res$v
print(v)
</code></pre>


</div>